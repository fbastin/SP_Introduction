\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usetheme{Singapore}
\usepackage{xcolor}
\setbeamertemplate{footline}[frame number]

%\usepackage{mathlist}

\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}

%\usepackage{epic,eepic}
%\usepackage{pstricks, pst-tree}

\usepackage{times}

\usepackage{ulem}
%\usepackage{crayola}

\usepackage{listings}
%\topmargin=-0.6in
\lstloadlanguages{C++}
\lstset{language=C++}
%%}

%\usepackage{gnuplottex}
% This variables allows to conditional generate the Gnuplot figures.
%\def \gnuplotx {Generate the figures}

\usepackage{graphicx}
\usepackage{epstopdf}

\setbeamercovered{dynamic}

\include{macros}

\def\bxi{\boldsymbol\xi}
\def\bepsilon{\boldsymbol\epsilon}
\def\bomega{\boldsymbol\xi}
\def\aff{\operatorname{aff}}
\def\cR{\mathcal{R}}

\newcommand{\tim}[1]{\;\; \mbox{#1} \;\;}

\title[CP]{Stochastic optimization\\Chance constrained programming}

\author[Fabian Bastin]{Fabian Bastin \\ \url{fabian.bastin@umontreal.ca} \\ Université de Montréal -- CIRRELT -- IVADO}

\date{}

\begin{document}

%% AJOUTER DISCUSSION BIRGE ET LOUVEAUX P128

\frame{\titlepage}

\begin{frame}
\frametitle{A long story}

\begin{itemize}
	\item 
Introduced in 1959 by Charnes and Cooper \url{https://dl.acm.org/doi/10.1287/mnsc.6.1.73}
	\item 
And also a bit improbable.
	\item 
Cooper dropped high-school to support his family, and became a professional boxer.
	\item 
Became an accountant for Eric Louis Kohler, met while hitchhiking.
	\item 
Kohler financed his bachelor at University of Chicago.
\item
At 26, he enrolled at Columbia University and finished his coursework and dissertation, but never received his PhD due to its claim that decision making was not a centralized process.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{A long story (cont'd)}
	
\begin{itemize}
\item
The collaboration with Charnes was however successful, with more than 200 publications, and led a successful academic carrer.
\item 
Source: \url{https://www.informs.org/Explore/History-of-O.R.-Excellence/Biographical-Profiles/Cooper-William-W}
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Cooper and Charnes}

\begin{center}
\includegraphics[width=0.4\textwidth]{imgs/coopercharnes.png}

INFORMS John Von Neumann prize (with Richard J. Duffin)
\end{center}

\end{frame}

\begin{frame}
\frametitle{Mathematical Formulation}

$$
\begin{aligned}
	\min_{x \in \calX} \quad & \mathbb{E}[f(x, \xi)] \\
	\text{s.t.} \quad & \mathbb{P}\left( g_i(x, \xi) \leq 0 \right) \geq 1 - \alpha_i, \quad i = 1, \ldots, m,
\end{aligned}
$$
where:
\begin{itemize}
\item \( \xi \in \mathbb{R}^p \) is a random vector with known probability distribution \( \mathbb{P} \);
	\item \( x \in \mathbb{R}^n \) is the vector of decision variables;
	\item \( \calX \subseteq \mathbb{R}^n \) is a convex deterministic set;
	\item \( f(x, \xi) \) is the cost (or utility) function;
	\item \( g_i(x, \xi) \) are random constraint functions;
	\item \( \alpha_i \in [0, 1] \) specifies the acceptable risk level for constraint \( i \).
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Probabilistic constraints}

The probabilistic constraint 
\[
\mathbb{P}\left( g_i(x, \xi) \leq 0 \right) \geq 1 - \alpha_i
\]
ensures that constraint \( i \) is satisfied with probability at least \( 1 - \alpha_i \). Equivalently, the probability of violation 
\[
\mathbb{P}\left( g_i(x, \xi) > 0 \right) \leq \alpha_i
\]
is limited to a user-specified tolerance. Smaller values of \( \alpha_i \) produce more conservative, risk-averse solutions.

\end{frame}

\begin{frame}
\frametitle{Individual and joint chance constraints}

Two common variants are:
\begin{enumerate}
	\item \textbf{Individual (or separate) chance constraints:} each constraint \( g_i(x, \xi) \leq 0 \) is required to hold with probability \( 1 - \alpha_i \);
	\item \textbf{Joint (or integrated) chance constraints:}
	\[
	\mathbb{P}\left( g_i(x, \xi) \leq 0, \, \forall i \right) \geq 1 - \alpha,
	\]
	which ensures simultaneous satisfaction of all constraints with a single confidence level \( 1 - \alpha \).
\end{enumerate}
%The joint form is more conservative and significantly harder to handle computationally.

\end{frame}

\begin{frame}
\frametitle{Toy example}

Source: J. Linderoth (\url{https://jlinderoth.github.io/classes/ie495/lecture22.pdf})

\mbox{}

Consider the toy problem
\begin{align*}
\min_x\ & x_1 + x_2 \\
\mbox{s.t. } & \xi_1 x_1 + x_2 \geq 7 \\
& \xi_2 x_1 + x_2 \geq 4 \\
& x_1, x_2 \geq 0.
\end{align*}

\mbox{} 

Instead of requiring that a constraint holds for all the scenarios, we can require that the constraint is satisfied with a given (large) probability.

\end{frame}

\begin{frame}
\frametitle{Chance constraints}

\begin{enumerate}
\item 
Separate chance constraints
\begin{align*}
P [ \xi_1x_1 + x_2 \geq 7 ] &\geq \alpha_1 \\
P [ \xi_2x_1 + x_2 \geq 4 ] &\geq \alpha_2
\end{align*}
\item
Joint (integrated) chance constraint
\[
P [ \xi_1x_1 + x_2 \geq 7 \cap \xi_2x_1 + x_2 \geq 4 ] \geq \alpha
\]
\end{enumerate}

\end{frame}

\begin{frame}
\frametitle{Example: joint chance constraints}

\begin{align}
P[(\xi_1, \xi_2) = (1,1)] &= 0.1 \\
P[(\xi_1, \xi_2) = (2,5/9)] &= 0.4 \\
P[(\xi_1, \xi_2) = (3,7/9)] &= 0.4 \\
P[(\xi_1, \xi_2) = (4,1/3)] &= 0.1
\end{align}

\mbox{}

Assume that $\alpha \in (0.8,0.9]$, and we have the joint constraint
\[
P [ \xi_1x_1 + x_2 \geq 7 \cap \xi_2x_1 + x_2 \geq 4 ] \geq \alpha
\]

\mbox{}

We then have to satisfy constraints (2) and (3) and either (1) or (4).

\end{frame}

\begin{frame}
\frametitle{Example: frontiers of constraints}

\begin{center}
\includegraphics<1>[width=\textwidth,keepaspectratio]{"chance_constraint_feasible_region.pdf"}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Feasible set}

Feasible set:
\[
K_1(\alpha) = \lbrace x \,|\, P[g_i(x,\bxi) \leq 0] \geq 1-\alpha_i,\ i,\ldots,m \rbrace
\]

\mbox{}

$K_1(\alpha)$ is not necessarily convex, even if $g_i(x,\bxi)$, $i,\ldots,m$, are linear in $x$.

\end{frame}

\begin{frame}
\frametitle{Quasi-concavity}

\begin{definition}[Quasi-Concavity]
	A function $f: S \to \mathbb{R}$, where $S \subseteq \mathbb{R}^n$ is a convex set, is \textbf{quasi-concave} if for all $\mathbf{x}, \mathbf{y} \in S$ and all $\lambda \in [0,1]$:
	\[
	f(\lambda\mathbf{x} + (1-\lambda)\mathbf{y}) \geq \min\{f(\mathbf{x}), f(\mathbf{y})\}
	\]
\end{definition}

\begin{definition}
The probability measure $P$ defined on the Borel sets of $\RR^n$ is
said to be quasi-concave, if for any convex subsets $A$, $B$ of $\RR^n$,
and any $\lambda \in [0,1]$,
\[
P[(1-\lambda)A+\lambda B] \geq \min \lbrace P[A], P[B] \rbrace.
\]
\end{definition}

(See Prékopa (2003), ``Probabilistic Programming'', Chapter 5 in ``\href{https://www.sciencedirect.com/handbook/handbooks-in-operations-research-and-management-science/vol/10}{Stochastic Programming}'')

\end{frame}

\begin{frame}
\frametitle{Applications in optimization}

\begin{theorem}[Convexity of Upper Level Sets]
	Let $f: S \to \RR$ be a quasi-concave function, where $S \subseteq \RR^n$ is a convex set. Then for any $\alpha \in \RR$, the upper level set
	\[
	U_\alpha = \{\bsx \in S \mid f(\bsx) \geq \alpha\}
	\]
	is convex.
\end{theorem}

\begin{corollary}[Feasible Set Convexity]
	If $f$ is quasi-concave, then the constraint:
	\[
	f(\bsx) \geq \alpha
	\]
	defines a \textbf{convex feasible set} for any $\alpha \in \mathbb{R}$.
\end{corollary}

\end{frame}

\begin{frame}
\frametitle{Applications in stochastic programming}

For chance-constrained programming problems of the form:
\[
P[g(\mathbf{x},\boldsymbol{\xi}) \leq 0] \geq 1-\alpha,
\]
if $P[g(\mathbf{x},\boldsymbol{\xi})\leq 0]$ is quasi-concave in $\bsx$, 
then $\{ \bsx \mid P[g(\bsx,\boldsymbol{\xi}) \leq 0] \geq 1-\alpha\}$ is \textbf{convex}, making the optimization problem \textbf{tractable}.

\end{frame}

\begin{frame}
\frametitle{Log-concavity}

\begin{definition}[Log-Concave Function]
	A function $f: S \to \mathbb{R}_+$, where $S \subseteq \mathbb{R}^n$ is a convex set, is \textbf{log-concave} if for all $\bsx, \bsy \in S$ and $\lambda \in [0,1]$:
	\[
	f(\lambda \mathbf{x} + (1-\lambda)\mathbf{y}) \geq f(\mathbf{x})^\lambda \, f(\mathbf{y})^{1-\lambda}
	\]
\end{definition}

\begin{theorem}
	If $f$ is log-concave and $f > 0$, then $f$ is quasi-concave.
\end{theorem}

\end{frame}

\begin{frame}
	\frametitle{Jointly Convex Constraint Function}

\begin{definition}
	A constraint function $g: \RR^n \times \RR^m \to \RR$ is \textbf{jointly convex} in $(\bsx, \bsxi)$ if for all $(\bsx_1, \bsxi_1), (\bsx_2, \bsxi_2) \in \RR^n \times \RR^m$ and all $\lambda \in [0,1]$:
	\[
	g(\lambda \bsx_1 + (1-\lambda)\bsx_2, \lambda \bsxi_1 + (1-\lambda)\bsxi_2) \leq \lambda g(\bsx_1, \bsxi_1) + (1-\lambda) g(\bsx_2, \bsxi_2)
	\]
\end{definition}

\end{frame}

\begin{frame}
\frametitle{Prékopa's fundamental theorem}

\begin{theorem}
Let $\bsxi \in \RR^m$ be a random vector with log-concave probability distribution, and let $g: \RR^n \times \mathbb{R}^m \to \mathbb{R}$ be quasi-convex in $(\mathbf{x}, \boldsymbol{\xi})$. Then the probability function
\[
	\phi(\mathbf{x}) = P[ g(\mathbf{x}, \boldsymbol{\xi}) \leq 0 ]
\]
is log-concave (and hence quasi-concave) in $\mathbf{x}$.
\end{theorem}

\mbox{}

(Prékopa, A. (1973). On logarithmic concave measures and functions.
Acta Scientiarum Mathematicarum 34, 335--343)
\end{frame}

\begin{frame}
\frametitle{Examples of jointly quasi-convex functions}

\begin{example}[Affine in $(\mathbf{x}, \boldsymbol{\xi})$]
	If $g(\mathbf{x}, \boldsymbol{\xi}) = \mathbf{a}^\top \mathbf{x} + \mathbf{b}^\top \boldsymbol{\xi} + c$, where $\mathbf{a} \in \mathbb{R}^n$, $\mathbf{b} \in \mathbb{R}^m$, $c \in \mathbb{R}$, then $g$ is jointly quasi-convex (in fact, jointly convex).
\end{example}

\begin{example}[Maximum]
The maximum of quasi-convex functions is quasi-convex.
\end{example}

\end{frame}

\begin{frame}
\frametitle{Common log-concave distributions}

\begin{align*}
	\text{Normal:} &\quad f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \\
	\text{Exponential:} &\quad f(x) = \lambda \exp(-\lambda x), \quad x \geq 0 \\
	\text{Uniform:} &\quad f(x) = \begin{cases}
		1/\mu(S), & x \in S \\
		0 & \mbox{otherwise},
	\end{cases}\\
& \text{where $\mu(S)$ is the measure of $S$}
	 \\
	\text{Gamma (shape $\geq 1$):} &\quad f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}, \quad x > 0, \alpha \geq 1 \\
	\text{Beta ($\alpha \geq 1, \beta \geq 1$):} &\quad f(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}, \quad x \in [0,1]\\
\text{Multivariate normal:} &\quad
f(x) = \frac{1}{\sqrt{(2\pi)^n/2\det(\Sigma)}}e^{-\frac{1}{2}(x-\mu)'\Sigma (x-\mu)}
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Quasi-concave probability distributions}

If you have such a density, you can
\begin{itemize}
\item
use Lagrangian techniques
\item
use a reduced-gradient technique (see Kall \& Wallace, Section~4.1)
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Joint constraints}

\begin{theorem}[Prékopa's Theorem for Joint Constraints]
	If
	\begin{enumerate}
		\item $\bsxi$ has a log-concave probability distribution
		\item Each $g_i(\bsx, \bsxi)$ is jointly quasi-convex in $(\bsx, \bsxi)$
		\item The set $\{(\bsx, \bsxi) \mid g_i(\bsx, \bsxi) \leq 0, \ i=1,\dots,m\}$ is convex
	\end{enumerate}
	Then the function
	\[
	\phi(\mathbf{x}) = P\left[ g_i(\mathbf{x}, \boldsymbol{\xi}) \leq 0, \ i=1,\dots,m \right]
	\]
	is log-concave (hence quasi-concave) in $\mathbf{x}$, and thus the feasible set $\{\mathbf{x} \mid \phi(\mathbf{x}) \geq 1-\alpha\}$ is convex.
\end{theorem}

\end{frame}

\begin{frame}
\frametitle{Maximum reformulation}

For joint constraints $P[g_j(\mathbf{x}, \boldsymbol{\xi}) \leq 0,\ \forall j] \geq 1-\alpha$, define:
\[
h(\mathbf{x}, \boldsymbol{\xi}) = \max_{j=1,\dots,m} g_j(\mathbf{x}, \boldsymbol{\xi})
\]
Then the constraint becomes:
\[
P[ h(\mathbf{x}, \boldsymbol{\xi}) \leq 0 ] \geq 1-\alpha
\]

\end{frame}

\begin{frame}
\frametitle{Further reading}

For a more complete analysis of convexity in chance-constrained programming, see Section~4.2 in \href{https://epubs.siam.org/doi/book/10.1137/1.9781611976595}{Lectures notes on stochastic programming} (3rd edition).

\end{frame}

\begin{frame}
	\frametitle{Linear inequalities}
	
Assume that the chance constraint can be written in the form
	$$
	P[T(\bxi)x \geq h(\bxi)] \geq \alpha.
	$$
	
	\begin{theorem}
		Suppose $T(\xi) = T$ is fixed, and $h(\bxi)$ has a quasi-concave
		probability measure $P$. Then $K_1(\alpha)$ is convex for $0 \leq \alpha \leq 1$.
	\end{theorem}
	
\end{frame}

\begin{frame}
\frametitle{Single constraint: easy case}

\begin{itemize}
	\item 
The situation in the single constraint case is sometimes simple.
\item 
Suppose again that $T_i (\xi) = T_i$ is constant. Then
\[
P[T_i x \geq h_i (\xi)] = F(T_i x) \geq \alpha
\]
so the deterministic equivalent is
\[
T_i x \geq F^{-1}(\alpha)
\]
\ldots linear constraint! The resulting problem is still linear. We have simply relaxed the contraint.
\item 
Recall that the inverse of the cdf is defined as
\[
F^{-1}(\alpha) = \min \lbrace x : F (x)  \geq \alpha \rbrace.
\]
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Other ``solvable'' cases}

Let $h(\xi) = h$ be fixed, $T(\xi) = \rm{diag}(\xi_1, \xi_2,\ldots, \xi_n)$, with $\xi = (\xi_1, \xi_2,\ldots, \xi_n)$ a multivariate normal distribution with mean $\mu = (\mu_1, \mu_2,\ldots,\mu_n)$ and variance-covariance matrix $\Sigma$.
Then
$$
\frac{\sum_{i = 1}^n \xi_i x_i - \mu^T x}{\sqrt{x^T\Sigma x}} \sim N(0,1),
$$
and
\[
K_1(\alpha) = \lbrace x \,|\, \mu^T x \geq h + \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} \rbrace,
\]
where $\Phi$ is the standard normal cdf.

\mbox{}

$K_1(\alpha)$ is a convex set for $\alpha \geq 0.5$.

\mbox{}

It is possible to express it as a second order cone constraint:
$$
\| \Sigma^{1/2} x \|_2 \leq \frac{1}{\Phi^{-1}(\alpha)} (\mu' x - h)
$$

\end{frame}

\begin{frame}
\frametitle{Second-order cone programming}

A second-order cone program (SOCP) is a convex optimization problem of the form
\begin{align*}
\min_x \ & f^T x \\
\mbox{s.t. } &
\| A_i x + b_i \|_2 \leq c_i^T x + d_i,\ i = 1,\ldots,m \\
& F x = g
\end{align*}
where $x \in \RR^n$, $f, c_i \in \RR^n$, $A_i \in \RR^{n_i \times n}$, $b_i \in \RR^{n_i}$, $d_i \in \RR$, $F \in \RR^{p \times n}$, and $g \in \RR^p$.

\mbox{}

SOCPs can be solved by interior point methods.

\end{frame}

\begin{frame}
\frametitle{Example: robust portfolio optimization}

(Taken from S. Boyd and J. Linderoth)

\begin{itemize}
	\item 
Suppose we want to invest in $n$ assets, providing random return rates $\beta_1, \beta_2,\ldots, \beta_n$.
\item
$\bsbeta \sim N(\bsmu, \Sigma)$.
\item
$x$: total amount to invest.
\item
Suppose that we want to ensure a return of at least $b$.
We cannot guarantee it all the time, but we want it to occur most of the time.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Example: robust portfolio optimization (cont'd)}

Let $x_i \geq 0$ the part of portfolio to invest in asset $i$.
Constraints:
\begin{align}
& P\left[ \sum_{i = 1}^n \beta_i x_i \geq b  \right] \geq \alpha \label{eq:var_constraint} \\
& \sum_{i = 1}^n x_i \leq x \nonumber \\
& x_i \geq 0,\ i = 1,\ldots,n.  \nonumber
\end{align}

\mbox{}

\eqref{eq:var_constraint} can be rewritten as
$$
\mu^Tx - \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} \geq b.
$$
If $b < 0$, \eqref{eq:var_constraint} is also known as \textcolor{red}{Value at Risk constraint} (Ruszczyński and Shapiro, 2003).


\end{frame}

\begin{frame}
\frametitle{Example: robust portfolio optimization (cont'd)}

We can also interpret $x_i$ as proportion of the portfolio (position of asset $i$), by normalizing $\| x \|_1$ to 1.
$b$ is now the minimum return rate of the portfolio and $x$ is the portfolio allocation.

\mbox{}

We can add some constraints on the $x_i$ to ensure diversification. We summarize them by requiring $x \in \mathcal{C}$.

\mbox{}

A complete program can now be expressed as
\begin{align*}
\max_x \ & E[\beta^T x] = \mu^T x \\
\mbox{s.t. } & P\left[ \beta^T x \geq b \right] \geq \alpha  \\
& \sum_{i = 1}^n x_i = 1 \\
& x \in \calC
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Example: loss constraint}

Setting $b$ to 0 means that we want to ensure that we will no suffer from loss with some probability. Typicially, $\alpha$ is set to 0.9, 0.95, 0.99,\ldots

\mbox{}

The chanced-constraint can also be expressed as
\[
P\left[ \beta^T x \leq 0 \right] \leq 1-\alpha = \gamma.
\]

\mbox{}

We can also allow the sale of some parts of the portfolio by allowing some $x_i$ to be negative.
\end{frame}

\begin{frame}
\frametitle{Numerical illustration}

(Taken from S. Boyd -- \url{http://ee364a.stanford.edu/lectures/chance_constr.pdf})

$n = 10$ assets, $\alpha = 0.95$, $\gamma = 0.05$, $\mathcal{C} = \{x | x \succeq -0.1\}$

\mbox{}

Compare
\begin{itemize}
\item
optimal portfolio
\item
optimal portfolio without loss risk constraint
\item
uniform portfolio $(1/n)\mathbf{1}$
\end{itemize}

\mbox{}

\begin{center}
\begin{tabular}{c|c|c}
portfolio & $E[\beta^T x]$ & $P[\beta^T x \leq 0])$ \\
\hline
optimal & 7.51 & 5.0\% \\
w/o loss constraint & 10.66 & 20.3\% \\
uniform & 3.41 & 18.9\%
\end{tabular}
\end{center}

\end{frame}

% Analytical solution if the non-negativity constraints are removed.
% See page 10 https://www.sciencedirect.com/science/article/pii/S0927050703100011

\begin{frame}
\frametitle{Short selling case}

Let's ignore the non-negativity constraints and consider the program (Ruszczyński and Shapiro, 2003)
\begin{align*}
	\min_x \ & -\mu^T x \\
	\text{s.t. } & \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} - \mu^Tx + b \leq 0.
\end{align*}

The Lagrangian is
\begin{align*}
L(x, \lambda) &= -\mu^T x + \lambda \left( \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} - \mu^Tx + b \right) \\
&= -(1+\lambda) \mu^T x + \lambda \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} + \lambda b
\end{align*}

\end{frame}

\begin{frame}
	\frametitle{Short selling case: KKT conditions}

\begin{align*}
&	\frac{dL(x, \lambda)}{dx} = 0 \\
&	\Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} - \mu^Tx + b \leq 0 \\
&	\lambda \left( \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} - \mu^Tx + b \right) = 0 \\
&	x \geq 0,\ \lambda \geq 0.
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Short selling case: solving the KKT conditions}

We have
$$
\frac{dL(x, \lambda)}{dx} = -(1+\lambda)\mu+\frac{\lambda \Phi^{-1}(\alpha)\Sigma x}{\sqrt{x^T\Sigma x}}
$$
If $\lambda = 0$,
$$
\frac{dL(x, \lambda)}{dx} = 0 \Rightarrow \mu = 0.
$$
Thus, wlog, we assume $\lambda \ne 0$. Therefore
$$
\Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} - \mu^Tx + b = 0
$$

\end{frame}

\begin{frame}
\frametitle{Short selling case: no risk-free asset}

(Ruszczyński and Shapiro, 2003)
Assume $\Sigma$ nonsingular and define
$$
\rho = \sqrt{ \mu^T \Sigma^{-1} \mu}
$$
We can show
$$
\begin{cases}
	\text{unbounded problem} & \text{if } \rho \geq \Phi^{-1}(\alpha); \\
	x^* = \frac{b}{\rho(\Phi^{-1}(\alpha)-\rho)} \Sigma^{-1} \mu & \text{if } \rho < \Phi^{-1}(\alpha).
\end{cases}
$$

\end{frame}

\begin{frame}
\frametitle{Numerical illustration}

\frametitle{Generalization}

A more general form is
\begin{align*}
	\min_x \ & h(x) \\
	\text{s.t. } & P[ g_1(x, \bxi) \leq 0,\ldots, g_r(x, \bxi) \leq 0] \geq \alpha \\
	& h_1(x) \leq 0,\ldots, h_m(x) \leq 0.
\end{align*}
or
\begin{align*}
	\min_x \ & h(x) \\
	\text{s.t. } & \EE\left[ \mathcal{I}_{(0,\infty)} \left( g_1(x, \bxi) \leq 0,\ldots, g_r(x, \bxi) \leq 0 \right) \right] \geq \alpha \\
	& h_1(x) \leq 0,\ldots, h_m(x) \leq 0,
\end{align*}
where
$$
\mathcal{I}_{(0,\infty)}(t) =
\begin{cases}
	1 & \mbox{if } t \leq 0,\\
	0 & \mbox{otherwise}.
\end{cases}
$$

\end{frame}

\begin{frame}
\frametitle{Solution methods for the general case}

\begin{itemize}
\item	
Usually very hard.
\item
Use a bounding approximation or sample average approximation (SAA).
\item
We will discuss about it in more details when introducing Monte Carlo techniques.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Probabilistic programming}

Source: András Prékopa (2003), ``Probabilistic Programming'', Chapter 5 in ``Stochastic Programming'', A. Ruszczy\'nski and A. Shapiro (editors), Elsevier.

\mbox{}

\begin{itemize}
	\item 
Sometimes we only want to maximize a probability.
\item
General form:
\begin{align*}
\max_x \ & P[ g_1(x, \bxi) \leq 0,\ldots, g_r(x, \bxi) \leq 0] \\
\text{subject to } & h_1(x) \leq 0,\ldots, h_m(x) \leq 0.
\end{align*}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Measures of violation}

\begin{itemize}
\item 
A chance constraint allows constraint violation with some probability.
\item
The violation can be large.
\item
It is often desirable to avoid too large violations.
\item
Can we penalize the violation?
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Value at Risk}

Source: \url{https://web.stanford.edu/class/ee364a/lectures/chance_constr.pdf}

\mbox{}

Value-at-risk of random variable $Z$, at level $\eta$:
$$
\text{VaR}(Z;\eta) = \inf \{ \gamma \,|\, P[Z \leq \gamma] \geq \eta \}
$$

\mbox{}

Therefore, the value-at-risk is simply the inverse of the cdf evaluated at $\eta$!
$$
\text{VaR}(Z;\eta) = F_Z^{-1}(\eta).
$$

\end{frame}

\begin{frame}
\frametitle{Conditional Value at Risk}

$$
\text{CVaR}(Z; \eta) = \inf_{\beta} \left( \beta + \frac{1}{1 - \eta} \EE \left[ (Z - \beta)_+ \right] \right).
$$

\mbox{}

Assume that the distribution of $Z$ is continuous.

\mbox{}

Solution $\beta^*$ obtained by solving
$$
0 = \frac{d}{d \beta} \left( \beta + \frac{1}{1 - \eta} \EE \left[ (Z - \beta)_+ \right] \right)
= 1 - \frac{1}{1 - \eta} P[ Z \geq \beta ],
$$
leading to
\begin{align*}
& P[ Z \geq \beta ] = 1 - \eta \\
%As $Z$ is continuous, this last equation can be rewritten as
\Leftrightarrow \quad & P[ Z \leq \beta ] = \eta = \text{VaR}(Z; \eta).
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Expected shortfall}

Conditional tail expectation (or expected shortfall)
\begin{align*}
\EE[z \,|\, z \geq \beta^* ]
&= \EE[\beta^* + (z - \beta^*) \,|\, z \geq \beta^*] \\
&= \beta^* + \frac{\EE\left[(z - \beta^*)_+\right]}{P[z \geq \beta^*]} \\
&= \text{CVaR}(z; \eta)
\end{align*}

\begin{itemize}
	\item Can be added to the objective.
	\item Can be used as a constraint: \textit{conditional expectation constraint}
	$$
	\EE[z \,|\, z \geq \beta^*] \leq d.
	$$
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Integrated chance constraints}

\begin{itemize}
\item 
Consider the stochastic constraints
$$
g_i(x, \xi) \leq 0,\ 1,\ldots, r.
$$
\item
Integrated chance constraint:
$$
\EE \left[ \max_i (g_i(x,\bxi))_+ \right] \leq d.
$$
\end{itemize}

\mbox{}

For more details, see Chapter 6, Willem K. Klein Haneveld, Maarten H. van der Vlerk, Ward Romeijnders (2020), ``Stochastic Programming - Modeling Decision Problems Under Uncertainty'', Springer.
\end{frame}

%% More to read in Prékopa, starting from Section 2.

\end{document}