\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usetheme{Singapore}
\usepackage{xcolor}
\setbeamertemplate{footline}[frame number]

%\usepackage{mathlist}

\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}

\newtheorem{defi}{Définition}
\newtheorem{theo}{Théorème}
\newtheorem{coro}{Corollaire}
\newtheorem{lem}{Lemme}

%\usepackage{epic,eepic}
%\usepackage{pstricks, pst-tree}

\usepackage{times}

\usepackage{ulem}
%\usepackage{crayola}

\usepackage{listings}
%\topmargin=-0.6in
\lstloadlanguages{C++}
\lstset{language=C++}
%%}

%\usepackage{gnuplottex}
% This variables allows to conditional generate the Gnuplot figures.
%\def \gnuplotx {Generate the figures}

\usepackage{graphicx}
\usepackage{epstopdf}

\setbeamercovered{dynamic}

\include{macros}

\def\bxi{\boldsymbol\xi}
\def\bepsilon{\boldsymbol\epsilon}
\def\bomega{\boldsymbol\xi}
\def\aff{\operatorname{aff}}
\def\cR{\mathcal{R}}

\newcommand{\tim}[1]{\;\; \mbox{#1} \;\;}

\title[CP]{Stochastic optimization\\Chance constrained programming}

\author[Fabian Bastin]{Fabian Bastin \\ \url{fabian.bastin@umontreal.ca} \\ Université de Montréal -- CIRRELT -- IVADO -- Fin-ML}

\date{}

\begin{document}

%% AJOUTER DISCUSSION BIRGE ET LOUVEAUX P128

\frame{\titlepage}

\begin{frame}
\frametitle{A long story}

\begin{itemize}
	\item 
Introduced in 1959 by Charnes and Cooper \url{https://dl.acm.org/doi/10.1287/mnsc.6.1.73}
	\item 
And also a bit improbable.
	\item 
Cooper dropped high-school to support his family, and became a professional boxer.
	\item 
Became an accountant for Eric Louis Kohler, met while hitchhiking.
	\item 
Kohler financed his bachelor at University of Chicago.
\item
At 26, he enrolled at Columbia University and finished his coursework and dissertation, but never received his PhD due to its claim that decision making was not a centralized process.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{A long story (cont'd)}
	
\begin{itemize}
\item
The collaboration with Charnes was however successful, with more than 200 publications, and led a successful academic carrer.
\item 
Source: \url{https://www.informs.org/Explore/History-of-O.R.-Excellence/Biographical-Profiles/Cooper-William-W}
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Cooper and Charnes}

\begin{center}
\includegraphics[width=0.4\textwidth]{imgs/coopercharnes.png}

INFORMS John Von Neumann prize (with Richard J. Duffin)
\end{center}

\end{frame}

\begin{frame}
\frametitle{Motivation}

Source: J. Linderoth \url{https://jlinderoth.github.io/classes/ie495/lecture22.pdf}

We consider the toy problem
\begin{align*}
\min_x\ & x_1 + x_2 \\
\mbox{s.t. } & \xi_1 x_1 + x_2 \geq 7 \\
& \xi_2 x_1 + x_2 \geq 4 \\
& x_1, x_2 \geq 0,
\end{align*}
where $\xi_1 \sim U(1,4)$, $\xi_1 \sim U(1/3,1)$.

\mbox{} 

Instead of requiring that a constraint holds for all the scenarios, we can require a sufficiently large probability to satisfy a constraint.

\end{frame}

\begin{frame}
\frametitle{Chance constraints}

\begin{enumerate}
\item 
Separate chance constraints
\begin{align*}
P [ \xi_1x_1 + x_2 \geq 7 ] &\geq \alpha_1 \\
P [ \xi_2x_1 + x_2 \geq 4 ] &\geq \alpha_2
\end{align*}
\item
Joint (integrated) chance constraint
\[
P [ \xi_1x_1 + x_2 \geq 7 \cap \xi_2x_1 + x_2 \geq 4 ] \geq \alpha
\]
\end{enumerate}

\end{frame}

\begin{frame}
\frametitle{Example: joint chance constraints}

\begin{align}
P[(\xi_1, \xi_2) = (1,1)] &= 0.1 \\
P[(\xi_1, \xi_2) = (2,5/9)] &= 0.4 \\
P[(\xi_1, \xi_2) = (3,7/9)] &= 0.4 \\
P[(\xi_1, \xi_2) = (4,1/3)] &= 0.1
\end{align}

\mbox{}

Assume that $\alpha \in (0.8,0.9]$, and we have the joint constraint
\[
P [ \xi_1x_1 + x_2 \geq 7 \cap \xi_2x_1 + x_2 \geq 4 ] \geq \alpha
\]

\mbox{}

We then have to satisfy constraints (2) and (3) and either (1) or (4).

\end{frame}

\begin{frame}
\frametitle{Example: frontiers of constraints}

\begin{center}
\includegraphics<1>[width=\textwidth,keepaspectratio]{graph.eps}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Properties}

Feasible set
\[
K_1(\alpha) = \lbrace x \,|\, P[T(\xi)x \geq h(\xi)] \geq \alpha \rbrace
\]
$K_1(\alpha)$ is not necessarily convex.

\mbox{}

\begin{theorem}
Suppose $T(\xi) = T$ is fixed, and $h(\xi)$ has a quasi-concave
probability measure $P$. Then $K_1(\alpha)$ is convex for $0 \leq \alpha \leq 1$.
\end{theorem}

\mbox{}

A function $P: D \rightarrow \mathcal{R}$ defined on a domain $D$ is quasi-concave if $\forall$ convex sets $U, V \subseteq D$, and $0 \leq \lambda \leq 1$,
\[
P[(1-\lambda)U+\lambda V] \geq \min \lbrace P[U], P[V] \rbrace.
\]

\end{frame}

\begin{frame}
\frametitle{Quasi-concave probability distributions}

\begin{itemize}
\item 
Uniform
\[
f(x) =
\begin{cases}
1/\mu(S), & x \in S \\
0 & \mbox{otherwise},
\end{cases}
\]
where $\mu(S)$ is the measure of $S$.
\item 
Exponential density
\[
f(x) = \lambda e^{-\lambda x}
\]
\item 
Multivariate normal density:
\[
f(x) = \frac{1}{\sqrt{(2\pi)^n/2\det(\Sigma)}}e^{-\frac{1}{2}(x-\mu)'\Sigma (x-\mu)}
\]
\end{itemize}

\mbox{}

If you have such a density, you can
\begin{itemize}
\item
use Lagrangian techniques
\item
use a reduced-gradient technique (see Kall \& Wallace, Section~4.1)
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Single constraint: easy case}

\begin{itemize}
	\item 
The situation in the single constraint case is sometimes simple.
\item 
Suppose again that $T_i (\xi) = T_i$ is constant. Then
\[
P[T_i x \geq h_i (\xi)] = F(T_i x) \geq \alpha
\]
so the deterministic equivalent is
\[
T_i x \geq F^{-1}(\alpha)
\]
\ldots linear constraint! The resulting problem is still linear. We have simply relaxed the contraint.
\item 
Recall that the inverse of the cdf is defined as
\[
F^{-1}(\alpha) = \min \lbrace x : F (x)  \geq \alpha \rbrace.
\]
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Other ``solvable'' cases}

Let $h(\xi) = h$ be fixed, $T(\xi) = \rm{diag}(\xi_1, \xi_2,\ldots, \xi_n)$, with $\xi = (\xi_1, \xi_2,\ldots, \xi_n)$ a multivariate normal distribution with mean $\mu = (\mu_1, \mu_2,\ldots,\mu_n)$ and variance-covariance matrix $\Sigma$.
Then
$$
\frac{\sum_{i = 1}^n \xi_i x_i - \mu^T x}{\sqrt{x^T\Sigma x}} \sim N(0,1),
$$
and
\[
K_1(\alpha) = \lbrace x \,|\, \mu^T x \geq h + \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} \rbrace,
\]
where $\Phi$ is the standard normal cdf.

\mbox{}

$K_1(\alpha)$ is a convex set for $\alpha \geq 0.5$.

\mbox{}

It is possible to express it as a second order cone constraint:
$$
\| \Sigma^{1/2} x \|_2 \leq \frac{1}{\Phi^{-1}(\alpha)} (\mu' x - h)
$$

\end{frame}

\begin{frame}
\frametitle{Second-order cone programming}

A second-order cone program (SOCP) is a convex optimization problem of the form
\begin{align*}
\min_x \ & f^T x \\
\mbox{s.t. } &
\| A_i x + b_i \|_2 \leq c_i^T x + d_i,\ i = 1,\ldots,m \\
& F x = g
\end{align*}
where $x \in \cR^n$, $f, c_i \in \cR^n$, $A_i \in \cR^{n_i \times n}$, $b_i \in \cR^{n_i}$, $d_i \in \cR$, $F \in \cR^{p \times n}$, and $g \in \cR^p$.

\mbox{}

SOCPs can be solved by interior point methods.

\end{frame}

\begin{frame}
\frametitle{Example: robust portfolio optimization}

(Taken from S. Boyd and J. Linderoth)

\begin{itemize}
	\item 
Suppose we want to invest in $n$ assets, providing random return rates $\beta_1, \beta_2,\ldots, \beta_n$.
\item
$\bsbeta \sim N(\bsmu, \Sigma)$.
\item
$x$: total amount to invest.
\item
Suppose that we want to ensure a return of at least $b$.
We cannot guarantee it all the time, but we want it to occur most of the time.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Example: robust portfolio optimization (cont'd)}

Let $x_i \geq 0$ the part of portfolio to invest in asset $i$.
Constraints:
\begin{align}
& P\left[ \sum_{i = 1}^n \beta_i x_i \geq b  \right] \geq \alpha \label{eq:var_constraint} \\
& \sum_{i = 1}^n x_i \leq x \nonumber \\
& x_i \geq 0,\ i = 1,\ldots,n.  \nonumber
\end{align}

\mbox{}

\eqref{eq:var_constraint} can be rewritten as
$$
\mu^Tx - \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} \geq b.
$$
If $b < 0$, \eqref{eq:var_constraint} is also known as \textcolor{red}{Value at Risk constraint} (Ruszczyński and Shapiro, 2003).


\end{frame}

\begin{frame}
\frametitle{Example: robust portfolio optimization (cont'd)}

We can also interpret $x_i$ as proportion of the portfolio (position of asset $i$), by normalizing $\| x \|_1$ to 1.
$b$ is now the minimum return rate of the portfolio and $x$ is the portfolio allocation.

\mbox{}

We can add some constraints on the $x_i$ to ensure diversification. We summarize them by requiring $x \in \mathcal{C}$.

\mbox{}

A complete program can now be expressed as
\begin{align*}
\max_x \ & E[\beta^T x] = \mu^T x \\
\mbox{s.t. } & P\left[ \beta^T x \geq b \right] \geq \alpha  \\
& \sum_{i = 1}^n x_i = 1 \\
& x \in \calC
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Example: loss constraint}

Setting $b$ to 0 means that we want to ensure that we will no suffer from loss with some probability. Typicially, $\alpha$ is set to 0.9, 0.95, 0.99,\ldots

\mbox{}

The chanced-constraint can also be expressed as
\[
P\left[ \beta^T x \leq 0 \right] \leq 1-\alpha = \gamma.
\]

\mbox{}

We can also allow the sale of some parts of the portfolio by allowing some $x_i$ to be negative.
\end{frame}

\begin{frame}
\frametitle{Numerical illustration}

(Taken from S. Boyd -- \url{http://ee364a.stanford.edu/lectures/chance_constr.pdf})

$n = 10$ assets, $\alpha = 0.95$, $\gamma = 0.05$, $\mathcal{C} = \{x | x \succeq -0.1\}$

\mbox{}

Compare
\begin{itemize}
\item
optimal portfolio
\item
optimal portfolio without loss risk constraint
\item
uniform portfolio $(1/n)\mathbf{1}$
\end{itemize}

\mbox{}

\begin{center}
\begin{tabular}{c|c|c}
portfolio & $E[\beta^T x]$ & $P[\beta^T x \leq 0])$ \\
\hline
optimal & 7.51 & 5.0\% \\
w/o loss constraint & 10.66 & 20.3\% \\
uniform & 3.41 & 18.9\%
\end{tabular}
\end{center}

\end{frame}

% Analytical solution if the non-negativity constraints are removed.
% See page 10 https://www.sciencedirect.com/science/article/pii/S0927050703100011

\begin{frame}
\frametitle{Short selling case}

Let's ignore the non-negativity constraints and consider the program (Ruszczyński and Shapiro, 2003)
\begin{align*}
	\min_x \ & -\mu^T x \\
	\text{s.t. } & \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} - \mu^Tx + b \leq 0.
\end{align*}

The Lagrangian is
\begin{align*}
L(x, \lambda) &= -\mu^T x + \lambda \left( \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} - \mu^Tx + b \right) \\
&= -(1+\lambda) \mu^T x + \lambda \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} + \lambda b
\end{align*}

\end{frame}

\begin{frame}
	\frametitle{Short selling case: KKT conditions}

\begin{align*}
&	\frac{dL(x, \lambda)}{dx} = 0 \\
&	\Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} - \mu^Tx + b \leq 0 \\
&	\lambda \left( \Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} - \mu^Tx + b \right) = 0 \\
&	x \geq 0,\ \lambda \geq 0.
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Short selling case: solving the KKT conditions}

We have
$$
\frac{dL(x, \lambda)}{dx} = -(1+\lambda)\mu+\frac{\lambda \Phi^{-1}(\alpha)\Sigma x}{\sqrt{x^T\Sigma x}}
$$
If $\lambda = 0$,
$$
\frac{dL(x, \lambda)}{dx} = 0 \Rightarrow \mu = 0.
$$
Thus, wlog, we assume $\lambda \ne 0$. Therefore
$$
\Phi^{-1}(\alpha) \sqrt{x^T\Sigma x} - \mu^Tx + b = 0
$$

\end{frame}

\begin{frame}
\frametitle{Short selling case: no risk-free asset}

(Ruszczyński and Shapiro, 2003)
Assume $\Sigma$ nonsingular and define
$$
\rho = \sqrt{ \mu^T \Sigma^{-1} \mu}
$$
We can show
$$
\begin{cases}
	\text{unbounded problem} & \text{if } \rho \geq \Phi^{-1}(\alpha); \\
	x^* = \frac{b}{\rho(\Phi^{-1}(\alpha)-\rho)} \Sigma^{-1} \mu & \text{if } \rho < \Phi^{-1}(\alpha).
\end{cases}
$$

\end{frame}

\begin{frame}
\frametitle{Numerical illustration}

\frametitle{Generalization}

A more general form is
\begin{align*}
	\min_x \ & h(x) \\
	\text{s.t. } & P[ g_1(x, \bxi) \leq 0,\ldots, g_r(x, \bxi) \leq 0] \geq \alpha \\
	& h_1(x) \leq 0,\ldots, h_m(x) \leq 0.
\end{align*}
or
\begin{align*}
	\min_x \ & h(x) \\
	\text{s.t. } & \EE\left[ \mathcal{I}_{(0,\infty)} \left( g_1(x, \bxi) \leq 0,\ldots, g_r(x, \bxi) \leq 0 \right) \right] \geq \alpha \\
	& h_1(x) \leq 0,\ldots, h_m(x) \leq 0,
\end{align*}
where
$$
\mathcal{I}_{(0,\infty)}(t) =
\begin{cases}
	1 & \mbox{if } t \leq 0,\\
	0 & \mbox{otherwise}.
\end{cases}
$$

\end{frame}

\begin{frame}
\frametitle{Solution methods for the general case}

\begin{itemize}
\item	
Usually very hard.
\item
Use a bounding approximation or sample average approximation (SAA).
\item
We will discuss about it in more details when introducing Monte Carlo techniques.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Probabilistic programming}

Source: András Prékopa (2003), ``Probabilistic Programming'', Chapter 5 in ``Stochastic Programming'', A. Ruszczy\'nski and A. Shapiro (editors), Elsevier.

\mbox{}

\begin{itemize}
	\item 
Sometimes we only want to maximize a probability.
\item
General form:
\begin{align*}
\max_x \ & P[ g_1(x, \bxi) \leq 0,\ldots, g_r(x, \bxi) \leq 0] \\
\text{subject to } & h_1(x) \leq 0,\ldots, h_m(x) \leq 0.
\end{align*}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Measures of violation}

\begin{itemize}
\item 
A chance constraint allows constraint violation with some probability.
\item
The violation can be large.
\item
It is often desirable to avoid too large violations.
\item
Can we penalize the violation?
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Value at Risk}

Source: \url{https://web.stanford.edu/class/ee364a/lectures/chance_constr.pdf}

\mbox{}

Value-at-risk of random variable $Z$, at level $\eta$:
$$
\text{VaR}(Z;\eta) = \inf \{ \gamma \,|\, P[Z \leq \gamma] \geq \eta \}
$$

\mbox{}

Therefore, the value-at-risk is simply the inverse of the cdf evaluated at $\eta$!
$$
\text{VaR}(Z;\eta) = F_Z^{-1}(\eta).
$$

\end{frame}

\begin{frame}
\frametitle{Conditional Value at Risk}

$$
\text{CVaR}(Z; \eta) = \inf_{\beta} \left( \beta + \frac{1}{1 - \eta} \EE \left[ (Z - \beta)_+ \right] \right).
$$

\mbox{}

Assume that the distribution of $Z$ is continuous.

\mbox{}

Solution $\beta^*$ obtained by solving
$$
0 = \frac{d}{d \beta} \left( \beta + \frac{1}{1 - \eta} \EE \left[ (Z - \beta)_+ \right] \right)
= 1 - \frac{1}{1 - \eta} P[ Z \geq \beta ],
$$
leading to
\begin{align*}
& P[ Z \geq \beta ] = 1 - \eta \\
%As $Z$ is continuous, this last equation can be rewritten as
\Leftrightarrow \quad & P[ Z \leq \beta ] = \eta = \text{VaR}(Z; \eta).
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Expected shortfall}

Conditional tail expectation (or expected shortfall)
\begin{align*}
\EE[z \,|\, z \geq \beta^* ]
&= \EE[\beta^* + (z - \beta^*) \,|\, z \geq \beta^*] \\
&= \beta^* + \frac{\EE\left[(z - \beta^*)_+\right]}{P[z \geq \beta^*]} \\
&= \text{CVaR}(z; \eta)
\end{align*}

\begin{itemize}
	\item Can be added to the objective.
	\item Can be used as a constraint: \textit{conditional expectation constraint}
	$$
	\EE[z \,|\, z \geq \beta^*] \leq d.
	$$
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Integrated chance constraints}

\begin{itemize}
\item 
Consider the stochastic constraints
$$
g_i(x, \xi) \leq 0,\ 1,\ldots, r.
$$
\item
Integrated chance constraint:
$$
\EE \left[ \max_i (g_i(x,\bxi))_+ \right] \leq d.
$$
\end{itemize}

\mbox{}

For more details, see Chapter 6, Willem K. Klein Haneveld, Maarten H. van der Vlerk, Ward Romeijnders (2020), ``Stochastic Programming - Modeling Decision Problems Under Uncertainty'', Springer.
\end{frame}

%% More to read in Prékopa, starting from Section 2.

\end{document}