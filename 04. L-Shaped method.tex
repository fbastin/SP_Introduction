\documentclass{beamer}
%\documentclass[xcolor=pst,dvips,epic,eepic]{beamer}

\usepackage[utf8]{inputenc}
\usetheme{Singapore}
\usepackage{xcolor}
\setbeamertemplate{footline}[frame number]

%\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
%\usepackage[pst]{xcolor}
%\usepackage{xxcolor}
\usepackage{mathlist}

%\usepackage[frenchb]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}

\newtheorem{theo}{Theorem}

\usepackage{pstricks}

%\usepackage{eepic}

\usepackage{times}

%\usepackage{pst-tree}

\usepackage{ulem}

\usepackage{listings}
%\topmargin=-0.6in
\lstloadlanguages{C++}
\lstset{language=C++}
%%}

\setbeamercovered{dynamic}

\include{macros}

\def\cF{\mathcal{F}}
\def\cK{\mathcal{K}}
\def\cB{\mathcal{B}}
\def\cQ{\mathcal{Q}}

\author[Fabian Bastin]{Fabian Bastin \\ \url{fabian.bastin@umontreal.ca} \\ Université de Montréal -- CIRRELT -- IVADO}
\date{}
\title[$L$-shaped method]{Stochastic optimization\\$L$-shaped method}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Two-stage linear program with recourse}

{\red Suggested reading: Section~3.2, Kall et Wallace}

\mbox{}

Summarize our formulations.
\[
\min_{x \in \RR^n_+ \,|\, Ax = b} \left\lbrace c^Tx + E_{\bsxi} \left[ \min_{y \in \RR^p_+} \lbrace q(\bsxi)^Ty \ |\ Wy = h(\bsxi) - T(\bsxi)x \rbrace \right] \right\rbrace 
\]
\[
\min_{x \in \RR^n_+ \,|\, Ax = b} \left\lbrace c^Tx + E_{\bsxi}
  \left[  v(\bsxi, h(\bsxi)-T(\bsxi)x) \right] \right\rbrace 
\]
\[
\min_{x \in \RR^n_+ \,|\, Ax = b} \left\lbrace c^Tx + E_{\bsxi} \left[ Q(x,\bsxi) \right] \right\rbrace 
\]
\[
\min_{x \in \RR^n_+} \left\lbrace c^Tx + \mathcal{Q}(x)
  \ |\ Ax = b \right\rbrace 
\]

We have studied its properties. But how to solve it?

\end{frame}

\begin{frame}
\frametitle{Notations: reminder}

\begin{itemize}
\item
First-stage feasible set:
\[
K_1 = \lbrace x \in \RR^n_+ \ |\ Ax = b \rbrace.
\]
\item
Second-stage feasible set:
\[
K_2^s = \lbrace x \ |\ \mathcal{Q}(x) < \infty \rbrace.
\]
\end{itemize}
We can therefore rewrite the problem as
\[
\min_x \lbrace c^Tx + \mathcal{Q}(x) \ |\ x \in K_1 \cap K_2^s \rbrace.
\]

%\mbox{}

%For simplicity, we will assume for now that the recourse is relatively complete.

\end{frame}

\begin{frame}
\frametitle{Deterministic equivalent}

Assume now that the support of $\bsxi$ is finite, and that we have $S$ scenarios.
The probability of scenario $s$ is $p_s$.

\mbox{}

The deterministic equivalent is
\begin{align*}
\min_{x \in X, y_1, \ldots, y_S}\ & c^T x + p_1 q^T y_1 + p_2 q^T y_2 + \ldots
p_S q^Ty_S \\
\mbox{s.t. } & \\
& \begin{matrix} Ax & & & & & = b\\
T_1 x & + W y_1 & & & & = h_1 \\
T_2 x & & + W y_2 & & & = h_2 \\
\vdots & & & \ddots & \\
T_S x & & & & + W y_S & = h_S
\end{matrix} \\
& x \in X,\ y_1 \in Y,\ y_2 \in Y,\ldots, y_S \in Y.
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Properties of the deterministic equivalent}

\begin{itemize}
\item
$y_s \overset{def}{=} y(\omega_s)$ is the recourse action to take if the scenario $s$ occurs.
\item
We obtain a linear program,\ldots but very large:
\begin{itemize}
\item
n + pS variables;
\item
m1 + mS constraints.
\end{itemize}
\item
The constraints has a very specific structure (staircase).
\item
{\red Idea}: use decomposition techniques.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Decomposition methods}

{\red General principle}: the nonlinear term (the recourse function) implies to compute the solution of all the second-stage recourse linear programs.

\mbox{}

Is is possible to avoid repeated evaluations of all the concerned functions in order to compute these solutions?

\mbox{}

{\red Idea}: build a master program in $x$, and evaluate the objective function exactly (including all the scenarios) as a subproblem only.

\end{frame}

\begin{frame}
\frametitle{Block structure}

\begin{minipage}{0.49\textwidth}
Primal:
$$
\begin{pmatrix}
A \\
T_1 & W \\
T_2 & & W \\
\vdots & & & \ddots \\
T_S & & & & W
\end{pmatrix}
$$
\end{minipage}
\begin{minipage}{0.49\textwidth}
Dual:
	$$
	\begin{pmatrix}
	A^T & T_1^T & T_2^T & \cdots & T_S^T \\
	& W^T \\
	& & W^T \\
	& & & \ddots \\
	& & & & W^T
	\end{pmatrix}
	$$
\end{minipage}

\mbox{}

The figures have give the name to the method: L-shaped.
\end{frame}

\begin{frame}
\frametitle{Main ideas}

{\red Section 1.6.4, Kall and Wallace}

Assume for now that $\Xi$ contains one element only.
The problem becomes
\begin{align*}
\min_{x,y}\ & c^Tx + q^Ty & \\
\mbox{s.t. } & Ax = b, \\
& Tx + Wy = h, \\
& x \geq 0,\ y \geq 0.
\end{align*}

\mbox{}

The problem can be reformulated as
\begin{align*}
\min_x\ & c^Tx + f(x) \\
\mbox{s.t. } & Ax = b, x \geq 0,
\end{align*}
with
\[
f(x) \overset{def}{=} \min_y \lbrace q^Ty \ |\ Wy = h - Tx,\ y \geq 0 \rbrace.
\]

\end{frame}

\begin{frame}
\frametitle{Problem reformulation}

Assume moreover that the problem is feasible and $\lbrace x \,|\, Ax = b,\ x \geq 0 \rbrace$ is bounded. This implies that
\[
\inf \lbrace f(x) \ |\ Ax = b,\ Tx+Wy =h,\ x \geq 0 \rbrace
\]
is finite and $f(x)$ can be uniformly bounded from below on $K_1$, say by $\theta$.

\mbox{}

If we explicitly know $f(x)$, we can write
\begin{align*}
\min_x\ & c^Tx + f(x) & \\
\mbox{t.q. } & Ax = b, \\
& \theta - f(x) \geq 0, \\
& x \geq 0.
\end{align*}

\end{frame}

\begin{frame}
\frametitle{First ideas}

Try to construct a {\blue sequence of new (additional) linear constraints} in order to define a feasible $\mathcal{B}_1$ of vectors $(x_1,\ldots,x_n,\theta)^T$ in $\RR^{n+1}$, such that, with
\[
\mathcal{B}_0 = \lbrace (x^T, \theta^T) \,|\, Ax = b,\ x \geq 0,\
\theta \in \RR \rbrace,
\]
the problem
\[
\min_{(x,\theta) \in \mathcal{B}_0 \cap \mathcal{B}_1} c^Tx+\theta
\]
gives a (first-stage) solution of the initial problem and an estimation $\theta$ of the second-stage objective value.

\end{frame}

\begin{frame}
\frametitle{Dual decomposition method}

\begin{description}
\item[\red Step 1]
Let $\theta_0$ be a lower bound for
\[
\min_y \lbrace q^Ty \,|\, Ax = b,\ Tx+Wy = h,\ x \geq 0,\ y \geq 0 \rbrace,
\]
and let $(\hat{x}, \hat{\theta})$ be a solution of
\[
\min_{x, \theta} \lbrace c^Tx + \theta \,|\, Ax = b,\ \theta \geq \theta_0,\ x
\geq 0 \rbrace.
\]
Set also $\mathcal{B}_1 := \lbrace \RR^n \times \lbrace \theta \rbrace
\,|\, \theta \geq \theta_0 \rbrace$.
\item[\red Step 2]
Evaluate
\begin{align*}
f(\hat{x}) &= \min_y \lbrace q^Ty \,|\, Wy = h-T\hat{x},\ y \geq 0
\rbrace \\
&= \max_{\pi} \lbrace (h-T\hat{x})^T\pi \,|\, W^T\pi \leq q \rbrace. \mbox{ (duality)}
\end{align*}
We have to consider two cases.
\end{description}
\end{frame}

\begin{frame}
\frametitle{Dual decomposition method (cont'd)}

{\red Step 2 (a)}
If $f(\hat{x}) = +\infty$, $\hat{x}$ is not feasible with all the second-stage constraints, there is no $y$ satisfying the linear system.

\mbox{}

\begin{block}{Farkas' lemma}
Let $A \in \RR^{m \times n}$ and $b \in \RR^m$.
Then, exactly one of the following two statements is true:
\begin{itemize}
\item
$\exists\, x \in \RR^n$ such that $Ax = b$ and $x \geq 0$;
\item
$\exists\, u \in \RR^m$ such that $u^T A \geq 0$ and $u^T b < 0$;
\end{itemize}
\end{block}
	
\mbox{}

Therefore, if $y$ is not feasible, $\exists\, u$ such that
\[
W^Tu \geq 0 \mbox{ et } (h - T\hat{x})^T u < 0.
\]
or, by setting $\sigma = -u$,
\[
W^T\sigma \leq 0 \mbox{ et } (h - T\hat{x})^T \sigma > 0.
\]

\end{frame}

\begin{frame}
\frametitle{Dual decomposition method (cont'd)}

Now, given $x$ feasible (for the first and second stage), there exists $y \geq 0$ s.t. $Wy = h-Tx$.
For such a $y$, we have
$$
\sigma^T(h-Tx) = \sigma^T W y \leq 0.
$$
as $W^T\sigma \leq 0$ and $y \geq 0$.
Therefore
\[
\sigma^Th \leq \sigma^TTx.
\]
This inequality should be valid for any feasible $x$ (therefore excluding $\hat{x}$), and we will enforce it by adding it to the first-stage linear program: {\red feasibility cut}
\[
\sigma^T(h-Tx) \leq 0.
\]
Redefine $\mathcal{B}_1 := \mathcal{B}_1 \cap \lbrace (x^T,
\theta) \,|\, \sigma^T(h-Tx) \leq 0 \rbrace$.

\end{frame}
%	\mbox{}
	
%	Recours relativement complet: $z \in \mbox{pos} W$ ou $\lbrace y \,|\,
%	Wy = z,\ y \geq 0\rbrace \ne \emptyset$.
	
%	Par le lemme de Farkas, cela équivaut à
%	$(W^Tu \geq 0) \Rightarrow (z^Tu \geq 0).$
	

\begin{frame}
\frametitle{Dual decomposition method (cont'd)}

{\red Step 2 (b)}
If $f(\hat{x})$ is finite, given $\hat{x}$, we have a primal optimal solution $\hat{y}$ and a dual optimal solution $\hat{\pi}$ with
\[
f(\hat{x}) = (h-T\hat{x})^T \hat{\pi}
\]
by the strong duality property.

\mbox{}

Moreover, for any $x$, we can write
\begin{align*}
f(x) &= \sup_{\pi} \left\lbrace (h-Tx)^T\pi \,|\, W^T\pi \leq q \right\rbrace \ \mbox{(second stage dual)}\\
&\geq (h-Tx)^T \hat{\pi} \\
&= \hat{\pi}^T(h-Tx).
\end{align*}

Since we want $\theta$ to represent $f(x)$, we would like
\[
\theta \geq \hat{\pi}^T(h-Tx).
\]

\end{frame}

\begin{frame}
\frametitle{Dual decomposition method (cont'd)}

$\theta \geq \hat{\pi}^T(h-Tx)$ is violated at $(\hat{x}, \hat{\theta})^T$
iff $\hat{\pi}^T(h-T\hat{x}) > \hat{\theta}$.

In this case we introduce the optimality cut, rejecting the suboptimal solution $(\hat{x}^T, \hat{\theta})^T$:
\[
\theta \geq \hat{\pi}^T (h-Tx),
\]
and we redefine $\mathcal{B}_1 := \mathcal{B}_1 \cap \lbrace (x^T,
\theta) \,|\, \theta \geq \hat{\pi}^T (h-Tx) \rbrace$.

\mbox{}

If $f(\hat{x}) \leq \hat{\theta}$, stop: $\hat{x}$ is a first-stage solution.
Otherwise, go to Step 3

\mbox{}

{\red Step 3}
Solve the updated problem
\[
\min \lbrace c^Tx + \theta \,|\, (x^T, \theta) \in \mathcal{B}_0 \cap
\mathcal{B}_1 \rbrace.
\]
Return to Step 2 with the new solution $(\hat{x}, \hat{\theta})$.

\end{frame}

\begin{frame}
\frametitle{Convergence}

{\blue Stopping rule}: $f(\hat{x}) \leq \hat{\theta}$.

Since
\begin{align*}
\hat{\theta} &= \max_{1 \leq i \leq k} (h-T\hat{x})^T\pi^i
\leq \max_{1 \leq i \leq r} (h-T\hat{x})^T\pi^i \\
&= f(\hat{x}) \leq \hat{\theta},
\end{align*}
the algorithm stops for $\hat{\theta}$ optimal.


\begin{block}{Proposition}
If the two-stage stochastic program is solvable and $\lbrace x
\,|\, Ax=b,\ x \geq 0 \rbrace$ is bounded, the dual decomposition method gives an optimal solution after a finite number of iterations.
\end{block}

\end{frame}

\begin{frame}
\frametitle{$S$ scenarios}

If $\Xi$ is finite, but is not a singleton?

\mbox{}

We can introduce feasibility and optimality cuts for all the recourse functions
\[
Q_{s}(x) = \min \left\lbrace (q_{s}^T y_{s} \,|\, Wy_{s} = h_{s}-T_{s}x,\ y_{s} \geq 0 \right\rbrace,
\]
$s = 1,\ldots,S$.
We obtain a {\sl \red multi-cut} dual decomposition method

\mbox{}

Alternatively, we can combine cuts corresponding to blocks $s = 1,\ldots,S$ with their respective probabilities.

\end{frame}

\begin{frame}
\frametitle{Single-cut algorithm}
	
For $S$, we can proceed as before, by remplacing $\mathcal{Q}(x)$ by $\theta$: singe-cut.
This leads to the {\red master program}
\begin{align*}
	\min c^Tx & + \theta \\
	\mbox{t.q. } Ax & = b, \\
	\theta &\geq \mathcal{Q}(x),\\
	x & \geq 0,\ \theta \in \RR.
\end{align*}
This program has $n+1$ variables only!

%\mbox{}

%Alternatively, we can consider cuts for a disjoint subsets of alternatives: multi-cut. We will consider this version later.

\end{frame}

\begin{frame}
\frametitle{Master program}
	
Denote by $(x^*, \theta^*)$ the optimal solution; we aim to have the constraint active on $\theta$:
\[
\theta^* = \mathcal{Q}(x^*).
\]
How to do? Use of cuts.
	
\mbox{}

A cut is an additional linear constraint, restraining the feasible space of the master program, but preserving the optimal solution of the original problem.
	
\mbox{}
	
As in the simple example, two types of cuts have to be considered:
\begin{itemize}
\item
\mbox{\red feasibility} cut;
\item
\mbox{\red optimality} cut.
\end{itemize}
	
\end{frame}

\begin{frame}
\frametitle{Feasibility cuts}

Solving the master problem can produce a solution $(\hat{x}, \hat{\theta})$ such that $\mathcal{Q}(\hat{x}) = +\infty$, i.e. $\hat{x} \notin K_2$.
%: $\hat{x}$ is not feasible for the second-stage program. No relatively complete recourse or complete recourse!
	
\mbox{}
	
The {\blue feasibility cuts} aim to keep the solutions of the master program in $K_1 \cap K_2$.
	
\mbox{}
	
\mbox{}

Recall that from Farkas lemma, we have
\begin{multline*}
\lbrace y \in \RR^p_+ \,|\, Wy = h-T\hat{x} \rbrace = \emptyset \\
\Rightarrow \exists\, \sigma \in \RR^m \mbox{ t.q. } W^T\sigma \leq 0
\mbox{ et } (h-T\hat{x})^T\sigma > 0.
\end{multline*}

\mbox{}

If our problem is well defined, we should have $K_1 \cap K_2 \ne \emptyset$.
For any feasible $x$, i.e. $x \in K_1 \cap K_2$, we know that $\exists y \geq 0$ such that $Wy = h-Tx$.
	
\end{frame}

\begin{frame}
\frametitle{Feasibility cuts (cont'd)}

We therefore aim to add the inequality
\[
\sigma^T(h(\xi_s)-T(\xi_s)x) \leq 0,
\]
where $s$ is a scenario for which $\hat{x}$ is not feasible.
	
\mbox{}
	
How to find $\sigma$? Basic ideas:
\begin{itemize}
\item
we have detected that the primal is not feasible as the dual is unbounded;
\item
therefore, there is some feasible direction along which we can increase the dual value as much as we want;
\item
this direction is the ray $\sigma$ that we are looking for.
\end{itemize}

\mbox{}

Lot of technical details in Birge \& Louveaux (Chapter 5). We will try to simplify and keep the main practical ideas.
	
\end{frame}

\begin{frame}
\frametitle{Feasibility cuts (cont'd)}

Similar to the two-phase method in linear programming.
The first phase looks for a feasible solution by solving an artificial program.
We do the same scenario by scenario, but what we look for is an infeasible scenario!

\mbox{}

For $s = 1,\ldots,S$, solve the linear program
\begin{align*}
\min\ & w = e^Tv^+ + e^Tv^- \\
\mbox{s.t. } & Wy + Iv^+ - Iv^- = h_s - T_sx^{\nu} \\
& y, v^+, v^- \geq 0,
\end{align*}
where $e^T = (1,\ldots,1)$, until, for some $s'$, %the optimal value
$w_* > 0$.

\mbox{}

As $v^+, v^- \geq 0$, $w_* \geq 0$;
\begin{itemize}
\item 
if $w^* = 0$, $v^+_* = v^-_* = 0$ and $Wy^* = h_s - T_sx^{\nu}$; in other terms the second-stage problem is feasible for $x^{\nu}$;
\item
if $w^* \ne 0$, $v^+_* \ne 0$ or $v^-_* \ne 0$, so $Wy^* \ne h_s - T_sx^{\nu}$, and we cannot find some $y$ such that $Wy = h_s - T_sx^{\nu}$.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Feasibility cuts (cont'd)}

The dual of the previous problem is
\begin{align*}
\max_{\sigma}\ & (h_s - T_sx^{\nu})^T\sigma \\
\mbox{s.t. } & W^T \sigma \leq 0 \\
& \sigma \leq e \\
& -\sigma \leq e
\end{align*}
or, equivalently,
\begin{align*}
\max_{\sigma}\ & (h_s - T_sx^{\nu})^T\sigma \\
\mbox{s.t. } & W^T \sigma \leq 0 \\
& \| \sigma \|_{\infty} \leq 1
\end{align*}

By duality, if $w_* \ne 0$, $(h_s - T_sx^{\nu})^T\sigma_* > 0$.
Moreover, as $W^T \sigma_* \leq 0$, $\sigma_*$ satisfies the conditions we were looking for!
\end{frame}

\begin{frame}
\frametitle{Example 4.2 B\&L, p. 174}

Consider the problem
\begin{align*}
	\min_x\ & 3x_1 + 2x_2 + E_{\bsxi}[\min_y -15y_1-12y_2] \\
	\mbox{s.t. } & 3y_1 + 2y_2 \leq x_1 \\
	& 2y_1 + 5y_2 \leq x_2 \\
	& 0.8 \xi_1 \leq y_1 \leq \xi_1 \\
	& 0.8 \xi_2 \leq y_2 \leq \xi_2 \\
	& x, y \geq 0
\end{align*}
The scenarios are
\[
\xi =
\begin{cases}
(4,6),\ p_1 = 0.25 \\
(4,8),\ p_2 = 0.25 \\
(6,6),\ p_3 = 0.25 \\
(6,8),\ p_4 = 0.25
\end{cases}
\]

\end{frame}

\begin{frame}
\frametitle{Deterministic equivalent}

{\red Interpretation}: we face two investment decisions $x_1$ and $x_2$,
that must allow to satisfy at least 80\% of the demand.

{\red Deterministic equivalent}:
\begin{align*}
\min_{x \geq 0, y \geq 0}\ & 3x_1 + 2x_2 - \sum_{s = 1}^4 \frac{1}{4} \left( 15y^{(s)}_1+12y^{(s)}_2 \right) \\
\mbox{s.t. } & 3y^{(s)}_1 + 2y^{(s)}_2 \leq x_1,\ s = 1,\ldots,4 \\
& 2y^{(s)}_1 + 5y^{(s)}_2 \leq x_2,\ s = 1,\ldots,4 \\
& 0.8 \xi^{(s)}_1 \leq y^{(s)}_1 \leq \xi^{(s)}_1,\ s = 1,\ldots,4 \\
& 0.8 \xi^{(s)}_2 \leq y^{(s)}_2 \leq \xi^{(s)}_2,\ s = 1,\ldots,4 \\
& x, y \geq 0
\end{align*}
where
$$
\xi^{(1)} = (4,6),\ 
\xi^{(2)} = (4,8), \
\xi^{(3)} = (6,6),\ 
\xi^{(4)} = (6,8)
$$

\end{frame}

\begin{frame}
\frametitle{Deterministic equivalent}

It is possible to solve the deterministic equivalent.

\mbox{}

We obtain the solution
\begin{align*}
(x_1^*, x_2^*) &= (27.2, 41.6) \\
\left(y^{(1)*}_1, y^{(1)*}_2\right) &= (4,6) \\
\left(y^{(2)*}_1, y^{(2)*}_2\right) &= (4,6.72) \\
\left(y^{(3)*}_1, y^{(3)*}_2\right) &= (5.0667,6) \\
\left(y^{(4)*}_1, y^{(4)*}_2\right) &= (4.8,6.4)
\end{align*}
for an expected optimal value of 22.44.

Is it possible to limit the expansion?

\end{frame}

\begin{frame}
	\frametitle{Example B\&L: master program}
	
	We now build the master program
	\begin{align}
	\min_{x, \theta}\ & 3x_1 + 2x_2 + \theta \\
	\mbox{s.t. } & x \geq 0.
	\end{align}
	Since there is currently no constraint on $\theta$, we simply ignore it.
	The solution is then $x^* = (0,0)$, but then, no scenario is feasible!
	
	\mbox{}
	
	In order to keep the discussion short, we will consider the scenario $s = 4$, with $\xi_4 = (6,8)$ (but in general, it is impossible to automatically choose the most convenient scenario).
	
\end{frame}

\begin{frame}
	\frametitle{Feasibility cut}
	
	In order to determine the feasibility cut, we have to solve the program
	\begin{align*}
	\min_{y,v} w = & v_1^-+v_2^-+v_3^++v_4^-+v_5^++v_6^- \\
	\mbox{s.t. } & 3y_1 + 2y_2 + v_1^+ - v_1^- = 0 \\
	& 2y_1 + 5y_2 + v_2^+ - v_2^- = 0 \\
	& y_1 + v_3^+ - v_3^- = 4.8 \\
	& y_1 + v_4^+ - v_4^- = 6.0 \\
	& y_2 + v_5^+ - v_5^- = 6.4 \\
	& y_2 + v_6^+ - v_6^- = 8.0 \\
	& y \geq 0,\ v \geq 0.
	\end{align*}
	The optimal value is $w^* = 11.2$, associated to $v^+_3 = 4.6$ and $v^+_5 = 6.4$, and the dual vector (obtained with glpk)
	\[
	\sigma^{\nu} = \begin{pmatrix} -1 & -1 & 1 & 0 & 1 & 0 \end{pmatrix}
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Feasibility cut (cont'd)}
	
	We define the feasibility cut
	\[
	D_1 x \geq d_1,
	\]
	with
	\[
	D_1 = (\sigma^{\nu})^T T
	= \begin{pmatrix} -1 & -1 & 1 & 0 & 1 & 0 \end{pmatrix}
	\begin{pmatrix}
	-1 &  0 \\
	0 & -1 \\
	0 &  0 \\
	0 &  0 \\
	0 &  0 \\
	0 &  0
	\end{pmatrix}
	= \begin{pmatrix} 1 & 1 \end{pmatrix}
	\]
	and
	\[
	d_1 = (\sigma^{\nu})^T h_1
	= \begin{pmatrix} -1 & -1 & 1 & 0 & 1 & 0 \end{pmatrix}
	\begin{pmatrix} 0 \\ 0 \\ 4.8 \\ 6.0 \\ 6.4 \\ 8.0 \end{pmatrix} = 11.2
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Feasibility cut (cont'd)}
	
	We obtain the cut
	\[
	x_1 + x_2 \geq 11.2
	\]
	which excludes $\hat{x} = (0,\ 0)$.
	
	\mbox{}
	
	Birge \& Louveaux reports the cut
	\[
	3x_1 + x_2 \geq 123.2
	\]
	Can we justify this difference?
	
	\mbox{}
	
	The dual solution is not unique, and we can also choose
	\[
	\sigma^{\nu} = \begin{pmatrix} -3/11 & -1/11 & 1 & 0 & 1 & 0 \end{pmatrix}^T
	\]
	$d_1$ does not change, but $D_1$ become $(3/11 \ 1/11)$.
	This solution makes the first two dual constraints active. In other terms, this solution corresponds to a different dual basis.
	
\end{frame}

\begin{frame}
\frametitle{Dual}	

\begin{align*}
\max_{\sigma^{\nu}}\ & 4.8\sigma_3^{\nu} + 6.0\sigma_4^{\nu} + 6.4\sigma_5^{\nu} + 8.0\sigma_6^{\nu} \\
\mbox{s.t. } & 3\sigma_1^{\nu} + 2 \sigma_2^{\nu} + \sigma_3^{\nu} + \sigma_4^{\nu} \leq 0  \quad (y_1)\\
& 2\sigma_1^{\nu} + 5 \sigma_2^{\nu} + \sigma_5^{\nu} + \sigma_6^{\nu} \leq 0 \quad (y_2)\\
& \sigma_1^{\nu} \leq 0 \quad (v_1^+),\ -\sigma_1^{\nu} \leq 1 \quad (v_1^-) \\
& \sigma_2^{\nu} \leq 0 \quad (v_2^+),\ -\sigma_2^{\nu} \leq 1 \quad (v_2^-) \\
& \sigma_3^{\nu} \leq 1 \quad (v_3^+),\ -\sigma_3^{\nu} \leq 0 \quad (v_3^-) \\ 
& \sigma_4^{\nu} \leq 0 \quad (v_4^+),\ -\sigma_4^{\nu} \leq 1 \quad (v_4^-) \\
& \sigma_5^{\nu} \leq 1 \quad (v_5^+),\ -\sigma_5^{\nu} \leq 0 \quad  (v_5^-) \\
& \sigma_6^{\nu} \leq 0 \quad (v_6^+),\ -\sigma_6^{\nu} \leq 1 \quad  (v_6^-) %\\ 
%& \sigma_1^{\nu} \leq 0,\ \sigma_2^{\nu} \leq 0,\ \sigma_3^{\nu} \geq 0,\ \sigma_4^{\nu} \leq 0,\ \sigma_5^{\nu} \geq 0,\ \sigma_6^{\nu} \leq 0
\end{align*}
	
\end{frame}


%\begin{frame}
%\frametitle{Dual (cont'd)}	

%Removing the redundant constraints, we obtain
%\begin{align*}
%\max_{\sigma^{\nu}}\ & 4.8\sigma_3^{\nu} + 6.0\sigma_4^{\nu} + 6.4\sigma_5^{\nu} + 8.0\sigma_6^{\nu} \\
%\mbox{s.t. } & 3\sigma_1^{\nu} + 2 \sigma_2^{\nu} + \sigma_3^{\nu} + \sigma_4^{\nu} \leq 0 \\
%& 2\sigma_1^{\nu} + 5 \sigma_2^{\nu} + \sigma_5^{\nu} + \sigma_6^{\nu} \leq 0 \\
%& -1 \leq \sigma_1^{\nu} \leq 0,\ -1 \leq \sigma_2^{\nu} \leq 0,\ 0 \leq \sigma_3^{\nu} \leq 1 %\\ 
%& -1 \leq \sigma_4^{\nu} \leq 0,\ 0 \leq \sigma_5^{\nu} \leq 1,\ -1 \leq \sigma_6^{\nu} \leq 0
%\end{align*}

%\end{frame}

\begin{frame}
\frametitle{Dual (cont'd)}	

By the complementarity conditions, $v_3^+ > 0$ and $v_5^+ > 0$ imply that $\sigma_3^{\nu} = 1$ and $\sigma_5^{\nu} = 1$. %, but nothing else can be said on the other dual variables as all the other primal variables are set to 0.
The strong duality requires that $\sigma_4^{\nu} = \sigma_6^{\nu} = 0$ in order to have the dual objective value 11.2.

\mbox{}

The optimal solutions can however differ on $\sigma_1^{\nu}$ and $\sigma_2^{\nu}$. %, that are however different from 0, so the first two primal constraints have to be active. The third an fifth primal constraints are active too.

\mbox{}


The solution
\[
\begin{pmatrix} -1 & -1 & 1 & 0 & 1 & 0 \end{pmatrix}
\]
sets two bound constraints active for $\sigma_1^{\nu}$ and $\sigma_2^{\nu}$
while the solution obtained by Birge and Louveaux make these constraints inactive, but the first two dual constraints are now active.

\end{frame}

\begin{frame}
	\frametitle{Feasibility cuts: B\&L results}

Use of CPLEX or Gurobi produce the B\&L cut, and
 the first-stage solution becomes $x^1 = (41.067,0)^T$.
	
	\mbox{}
	
If we continue as in B\&L, a second feasibility cut is generated: $x_2 \geq 22.4$.
The first-stage solution becomes $x^1 = (33.6, 22.4)^T$.
	
	\mbox{}
	
	A third feasibility cut is generated: $x_2 \geq 41.6$.
	The first-stage decision is now $x^1 = (27.2, 41.6)^T \in \cK_1 \cap \cK_2$.
	
	\mbox{}
	
	Another choice for the initial realization would have produced different cuts.
	
\end{frame}

\begin{frame}
	\frametitle{On the problem formulation}
	
	The production of feasibility cuts can be inefficient.
	
	\mbox{}
	
	With $\xi_1 = 6$ and $\xi_2 = 8$, it is easy to see that we must have $y_1 \geq	4.8$ and $y_2 \geq 6.4$.
	
	\mbox{}
	
	From the first two second-stage constraints, this implies $x_1 \geq 27.2$ and $x_2 \geq 41.6$.
	
	\mbox{}
	
	We could therefore consider the following initial program
	\begin{align*}
	\min\ & 3x_1 + 2x_2 + \mathcal{Q}(x) \\
	\mbox{s.t. } & x_1 \geq 27.2, \\
	& x_2 \geq 41.6.
	\end{align*}
	
	The recourse is now relatively complete!
	
\end{frame}


\begin{frame}
	\frametitle{Optimality cuts}
	
	The problem
	\begin{align*}
		\min\ & c^Tx + \mathcal{Q}(x) \\
		\mbox{s.t. } & x \in K_1 \cap K_2,
	\end{align*}
	is equivalent to
	\begin{align*}
		\min\ & c^Tx + \theta \\
		\mbox{s.t. } & \mathcal{Q}(x) \leq \theta, \\
		& x \in K_1 \cap K_2,
	\end{align*}
	
The constraint $\theta \geq \mathcal{Q}(x)$ cannot be directly used, and will be expressed by means of optimality cuts.

\end{frame}

\begin{frame}
\frametitle{Master problem}
	
Initially, the only constraint on $\theta$ is $\theta \in \RR$, and we obtain the infimum of the objective with $\theta = -\infty$.	

% Initially, there is no cut

\mbox{}

As long as $\theta = -\infty$, we remove it from the objective function.
In other terms, we solve the first-stage problem only.

%Rewrite the master program
%\begin{align*}
%\min c^Tx & + \theta \\
%\mbox{s.t. } Ax & = b, \\
%\theta &\geq \mathcal{Q}(x),\\
%x & \geq 0,\ \theta \in \RR.
%\end{align*}
	
	\mbox{}

{\blue Temporarily ignore} this constraint.
We obtain an optimal solution $(\hat{x}, \theta)^T$, with $\theta$ set to $-\infty$ during the first iteration of the method.

\mbox{}

The structure of the deterministic equivalent is exploited by minimizing each second-stage function separately, and the decomposition is based on the primal problem.
Therefore we speak of primal decomposition, or Benders decomposition.	

\end{frame}

\begin{frame}
\frametitle{Optimality cut}
	
Consider the expected second-stage cost
\[
\mathcal{Q}(x) = \sum_{j=1}^S p_jQ(x,\xi_j),
\]
with
\[
Q(x,\xi_j) = \min \lbrace q(\xi_j)^Ty \,|\, Wy = h(\xi_j)-T(\xi_j)x,\
y \geq 0 \rbrace.
\]
We know that $Q(x,\xi_j)$ is convex and piecewise linear, and since $\Xi$ is finite, $\mathcal{Q}$ is also convex and piecewise linear.
	
%\mbox{}
	
%As in the simplified problem, we reformulate the problem by introducing a variable $\theta$:
%\begin{align*}
%\min_{x,\theta}\ & c^Tx+\theta\\
%\mbox{s.t. } & Ax = b, x \geq 0,\\
%& \theta \geq \mathcal{Q}(x),\ 
%\sigma_k^T(h(\xi_k)-T(\xi_k)x) \leq 0,\ k = 1,\ldots,K.\\
%\end{align*}
	
\end{frame}

\begin{frame}
	\frametitle{Optimality cut (cont'd)}
	
	If {\red the second-stage problem is feasible} for a given $\xi$, we can consider its dual
	\[
	\max \lbrace \pi^T(h(\xi)-T(\xi)x) \,|\, \pi^TW \leq q(\xi)^T \rbrace.
	\]
	
	\begin{itemize}
		\item 
		If $q(\xi)$ is fixed, the dual is feasible or unfeasible for all $x$ and $\xi$, as $x$ and $\xi$ do not enter the constraints.
		\item
		If $q$ depends on the realization $\xi$, given $\xi$, if the dual is feasible for some $x$, it is feasible for any value of $x$.
		\item
		From the duality properties, the primal is unbounded implies that the dual is not feasible.
		Assume that we know that our problem is bounded.
	\end{itemize}
	
	
\end{frame}

\begin{frame}
\frametitle{Subproblems}
	
Consider the master problem solution $(\hat{x}, \hat{\theta})$. We have
\[
\mathcal{Q}(\hat{x}) = \sum_j p_j Q(\hat{x}, \xi_j) = \sum_j p_jq(\xi_j)^Ty_j,
\]
where $y_j$ is the second-stage optimal solution associated to $\xi_j$.
	
\mbox{}

From the duality principle, we also have	
\[
\sum_j p_j q(\xi_j)^T y_j = \sum_j p_j (\hat{\pi}_j)^T(h(\xi_j) -
T(\xi_j)\hat{x}),
\]
where $\hat{\pi}_j$ is the dual optimal solution associated to $\xi_j$.
	
\mbox{}
	
The constraints in the dual problem are $\pi^T W \leq q(\xi_j)$, and are independent of $x$.
	
\end{frame}

\begin{frame}
\frametitle{Subproblems (cont'd)}
	
For any $x$ in $\cK_1 \cap \cK_2$, we have
\begin{align*}
\mathcal{Q}(x) &= \max_{\pi} \sum_j p_j \pi_j^T(h(\xi_j) - T(\xi_j)x) \\
& \geq \sum_j p_j(\hat{\pi}_j)^T(h(\xi_j) - T(\xi_j)x),
\end{align*}
	
\mbox{}
	
The constraint $\theta \geq \mathcal{Q}(x)$ implies
\[
\theta \geq \sum_j p_j(\hat{\pi}_j)^T(h(\xi_j) - T(\xi_j)x) = \alpha +
\beta^Tx,
\]
or
\[
-\beta^Tx + \theta \geq \alpha,
\]
with $\alpha = \sum_j p_j(\hat{\pi}_j)^Th(\xi_j)$ and
$\beta = -\sum_j T(\xi_j)^Tp_j(\hat{\pi}_j)$.
	
\end{frame}

\begin{frame}
\frametitle{Master problem}

Taking account of feasibility and optimality cuts, the master problem can be formulated as:
\begin{align*}
\min\ & c^Tx + \theta \\
\mbox{s.t. } & Ax = b, \\
& D_{\ell}^Tx \geq d_{\ell},\ \ell = 1,\ldots,K, & \mbox{(feasibility cuts)} \\
& -\beta_{\ell}^Tx + \theta \geq \alpha_{\ell},\ \ell = 1,\ldots,L, & \mbox{(optimality cuts)}\\
& x \geq 0,\ \theta \in \RR.
\end{align*}
%where $K$ and $L$ represent the number of feasibility and optimality cuts, respectively, generated during the previous iterations.
	
\mbox{}

Let $(x^{\nu}, \theta^{\nu})$ be an optimal solution, with $\nu$ the iteration index.
If no optimality cut is present, $\theta^{\nu}$ is set to $-\infty$ and is not considered in the computation of $x^{\nu}$.

\end{frame}

\begin{frame}
\frametitle{$L$-shaped algorithm}
	
\begin{description}
\item[Step 0.]  {\red Initialization}:
$\nu := 0$, $K := 0$, $L:= 0$, $\theta^{0} := -\infty$.
\item[Step 1.] {\red Solve the master problem} in order to get $x^0$.
Stop if the program is not feasible ($K_1 = \emptyset$).
\item[Step 2.]
Generate a {\red feasibility cut}.
For $s = 1,\ldots,S$, solve the linear problem
\begin{align*}
\min\ & w = e^Tv^+ + e^Tv^- \\
\mbox{s.t. } & Wy + Iv^+ - Iv^- = h_s - T_sx^{\nu} \\
& y, v^+, v^- \geq 0,
\end{align*}
where $e^T = (1,\ldots,1)$, until we find a strictly positive optimal value for some scenario $s$.
\end{description}
	
\end{frame}

\begin{frame}
\frametitle{$L$-shaped algorithm (cont'd)}
	
\begin{description}
\item[Step 2.] (continuation)
Let $\sigma^{\nu}$ be the optimal solution of the dual program, and define
\begin{align*}
D_{K+1} &= (\sigma^{\nu})^TT_s, \\
d_{K+1} &= (\sigma^{\nu})^Th_s,
\end{align*}
in order to generate a feasibility cut.
Increment $K$ and go to Step 4.

\mbox{}
		
If for all $s$, $w = 0$, go to Step 3.
\end{description}
	
\end{frame}

\begin{frame}
\frametitle{$L$-shaped algorithm (cont'd)}
	
\begin{description}
\item[Step 3.]
Compute $\mathcal{Q}(x^{\nu})$, by solving each second-stage problem: for $s = 1,\ldots,S$, solve
\begin{align*}
\min\ & q^T_sy \\
\mbox{t.q. } & Wy = h_s-T_sx^{\nu},\\
& y \geq 0.
\end{align*}
Stop if $\theta^{\nu} \geq \mathcal{Q}(x^{\nu})$: $x^{\nu}$ is an optimal solution.
		
Otherwise, increment $L$ and build the optimality cut $-\beta_L^T
x + \theta \geq \alpha_L$.
\item[Step 4.]
Increment $\nu$ and solve the master program to get $x^{\nu}$.
Stop if the program is not feasible. Otherwise, go to step 2.
\end{description}
	
\end{frame}

\begin{frame}
\frametitle{Remarks}

A pseudo-code is also available in Kall and Wallace.
	
%\mbox{}
	
%{\red Stopping criteria.}
%If $(x^{\nu}, \theta^{\nu})$ is optimal, it follows $\mathcal{Q}(x^{\nu}) = \theta^{\nu}$, as $\theta$ is not restricted, except for $\theta \geq \mathcal{Q}(x)$.
	
%This occurs when $\theta^{\nu} = E[\boldsymbol{\boldsymbol{b}+\boldsymbol{T}x^{\nu}]$.

\mbox{}
	
\begin{theorem}
If $\xi$ is a random vector with finite moments of order 2, the	$L$-shaped algorithm converges in a finite number of iterations towards an optimal solution if it exists, or establishes that the two-stage (linear, with fixed recourse) stochastic program is not feasible.
\end{theorem}
	
\end{frame}

\begin{frame}
	\frametitle{Subgradients and optimality cuts}
	
	We can also interpret the optimality cuts in terms of subgradients.
	Recall that $\mathcal{Q}(x)$ is convex and piecewise linear, so not differentiable everywhere.
	
	\mbox{}
	
	We can however show that the optimality cuts are support hyperplans of $\mathcal{Q}(x)$, by means of subgradients, and the feasibility cuts adequately define the feasible points in $K_2$.
	
	
\end{frame}

\begin{frame}
\frametitle{Subgradients}
	
\begin{definition}[Subgradient]
	A vector $\eta \in \RR^n$ is a {\red subgradient} of a convex function $f$ at $x$ iff
\[ f(y) \geq f(x) + \eta^T(y-x),\ \forall y \in \RR^n.\]
\end{definition}

\begin{definition}[Subdifferential]
	The set of all subgradients of $f$ at $x$ is called the {\red subdifferenntial} of $f$ at $x$ and is denoted by $\partial f(x)$.
\end{definition}

\mbox{}

	The graph of the (linear) function $h(y) = f(x) + \eta^T(y-x)$ is a support hyperplan for the convex set $epi(f) = \{ (x, \mu) \, : \, x \in \mathbb{R}^n,\, \mu \in \mathbb{R},\, \mu \ge f(x) \} \subseteq \mathbb{R}^{n+1}$, at $(x, f(x))$.


\end{frame}

\begin{frame}
	\frametitle{Subgradients (cont'd)}
	
	Recall that, by duality,
	\[
	Q(\hat{x}, \xi_s) =
	\max_{\pi} \left\lbrace \pi^T(h(\xi_s)-T(\xi_s)x) \,|\, \pi^TW \leq q(\xi_s)^T \right\rbrace.
	\]
	Therefore, if $x \in \cK_1 \cap \cK_2$, $\exists\ \hat{\pi}$ such that
	\[
	Q(\hat{x}, \xi_s) = \hat{\pi}^Th(\xi_s)-\hat{\pi}^TT(\xi_s)\hat{x}.
	\]
	With respect to the dual variables,
	\[
	Q(x, \xi_s) \geq \hat{\pi}^Th(\xi_s)-\hat{\pi}^TT(\xi_s) x,
	\ \forall x \in K_2(\xi_s)
	\]
	and thus
	\[
	Q(x, \xi_s) \geq Q({\hat{x}, \xi_s}) - \hat{\pi}^TT(\xi_s) (x-\hat{x}),
	\ \forall x \in K_2(\xi_s).
	\]
	
	In other words, $-T(\xi_s)^T\hat{\pi} \in \partial Q(\hat{x}, \xi_s)$.
	
	\mbox{}
	
	Similarly, by taking the expectation, $- \sum_s p_s T(\xi_s)^T\hat{\pi}_s \in \partial \mathcal{Q}(x)$.
	
\end{frame}

\begin{frame}
\frametitle{Optimality cuts: example}
	
{\red Inspired by Birge et Louveaux}
	
We consider the problem
\begin{align*}
\min_x\ & c^Tx + E_{\bsxi} [ Q(x,\xi) ],\\
\mbox{s.t. } & 0 \leq x \leq 10.
\end{align*}
Assume that $c = 0$, so that the program can be reduced to the second stage, with a second-stage problem defined as
\begin{align*}
\min y^++y^- \mbox{ t.q. } y^+-y^- = \xi - x,\ y^+ \geq 0,\ y^- \geq 0.
\end{align*}
It is possible to analytically compute $Q(x,\xi)$, that can be expressed as:
\[
Q(x,\xi) =
\begin{cases}
\xi - x,\quad \mbox{si } x \leq \xi,\\
x - \xi,\quad \mbox{si } x \geq \xi.
\end{cases}
\]
	
\end{frame}

\begin{frame}
\frametitle{$\mathcal{Q}(x)$}
	
What about $\mathcal{Q}(x)$?
	
\mbox{}
	
Assume that the three realizations have the same probability.
	
\mbox{}
	
A previous analysis showed that we have to examine 4 cases:
\begin{enumerate}
\item
$x \leq 1$: $\mathcal{Q}(x) = 7/3 - x$;
\item
$1 \leq x \leq 2$: $\mathcal{Q}(x) = 5/3-x/3$;
\item
$2 \leq x \leq 4$: $\mathcal{Q}(x) = x/3+1/3$;
\item
$4 \leq x$: $\mathcal{Q}(x) = x-7/3$;
\end{enumerate}
	
\end{frame}

\begin{frame}
\frametitle{Application of the $L$-shaped algorithm}
	
We assume that the starting point is $x^1 = 0$.
The sequence of iterates for the $L$-shaped method is as follows.

\mbox{}
	
\begin{description}
\item[Iteration 1] $x^1$ is not optimal; create the cut $\theta \geq 7/3 - x$.
\item[Iteration 2] $x^2 = 10$, $\theta^2 = -23/3$ are not optimal; create the cut $\theta \geq x- 7/3$.
\item[Iteration 3] $x^3 = 7/3$, $\theta^3 = 0$ are not optimal; create the cut $\theta \geq (x+1)/3$.
\item[Iteration 4] $x^4 = 1.5$, $\theta^4 = 2.5/3$ are not optimal; create the cut $\theta \geq (5-x)/3$.
\item[Iteration 5] $x^5 = 2$, $\theta^5 = 1$, which is the optimal solution.
\end{description}
	
\end{frame}

\begin{frame}
	\frametitle{$\mathcal{Q}(x)$, no cut}
	
	\includegraphics[width=0.9\textwidth]{all.eps}
	
\end{frame}

\begin{frame}
	\frametitle{$\mathcal{Q}(x)$, addition of the cut $\theta \geq 7/3-x$}
	
	\includegraphics[width=0.9\textwidth]{coupe_1.eps}
	
\end{frame}

\begin{frame}
	\frametitle{$\mathcal{Q}(x)$, addition of the cut $\theta \geq x-7/3$}
	
	\includegraphics[width=0.9\textwidth]{coupe_2.eps}
	
\end{frame}

\begin{frame}
	\frametitle{$\mathcal{Q}(x)$, addition of the cut $\theta \geq (x+1)/3$}
	
	\includegraphics[width=0.9\textwidth]{coupe_3.eps}
	
\end{frame}

\begin{frame}
\frametitle{$\mathcal{Q}(x)$, addition of the cut $\theta \geq (5-x)/3$}
	
\includegraphics[width=0.9\textwidth]{coupe_4.eps}
	
\end{frame}

\begin{frame}
\frametitle{Multi-cut version}

\begin{itemize}
	\item
	Why do we build one cut only?
	{\red Idea}: approximate {\blue each} $Q(x,\xi_s)$ by an auxiliary variable $\theta_s$, $s \in S$.
	\item
	We new introduce the cuts
	\[
	\theta_s \geq p_s(\hat{\pi}_s)^T(h(\xi_s) - T(\xi_s)x) = \alpha_s +
	\beta_s^Tx,\ s = 1 \in S.
	\]
	In other terms, we keep $|S|$ cuts instead of aggregating them.
	If it is not possible to build these $|S|$ cuts, the solution is optimal.
	\item
	Which method works the best?
	The multi-cut version offers a more detailed information, so in general, we can expect less major iterations, but as more cuts are added, the first-stage program is more expensive to solve.
	As a rule of thumb, the multi-cut approach is more efficient when the number of realizations $S$ is not significantly greater than the number of first-stage constraints.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Reminder: subgradients}

Recall that, by {\blue duality},
\[
Q(\hat{x}, \xi_s) =
\max_{\pi} \lbrace \pi^T(h(\xi_s)-T(\xi_s)\hat{x}) \,|\, \pi^TW \leq q(\xi_s)^T) \rbrace.
\]
Therefore, $\exists\ \hat{\pi}$ t.q.
$Q(\hat{x}, \xi_s) = \hat{\pi}^Th(\xi_s)-\hat{\pi}^TT(\xi_s)\hat{x}$.
With respect to the dual variables, we have
\[
Q(x, \xi_s) \geq \hat{\pi}^Th(\xi_s)-\hat{\pi}^TT(\xi_s) x,
\ \forall x \in K_2(\xi_s)
\]
and therefore
\[
Q(x, \xi_s) \geq Q({\hat{x}, \xi_s}) - \hat{\pi}^TT(\xi_s) (x-\hat{x}),
\ \forall x \in K_2(\xi_s).
\]

In other terms, $-T(\xi_s)^T\hat{\pi} \in \partial Q(\hat{x}, \xi_s)$.

\mbox{}

Similarly, by taking the expectation, $- \sum_s p_s T(\xi_s)^T\hat{\pi}_s \in \partial \mathcal{Q}(x)$.

\end{frame}

\begin{frame}
\frametitle{Multicut $L$-shaped method}
	
\begin{description}
\item[Step 0]
With $\theta_s^0$ a lower bound for $Q(x, \xi_s)$, define
\begin{align*}
\mathcal{B}_0 &= \lbrace \RR_n^+ \times \lbrace \theta_1,
\theta_2,\ldots, \theta_{|S|}\rbrace \,|\, Ax = b \rbrace, \\
\mathcal{B}_1 &= \lbrace \RR_n^+ \times \lbrace \theta_1,
\theta_2,\ldots, \theta_{|S|}\rbrace \,|\, \theta_s \geq \theta_s^0,\
\forall s \in S \rbrace.
\end{align*}
Set $\nu = 0$.
\item[Step 1]
Increment $\nu$.
Solve the master problem
\[
\min \left\lbrace c^Tx+\sum_{s \in S} \theta_s \,|\,
(x,\theta_1,\theta_2,\ldots,\theta_{|S|}) \in \mathcal{B}_0 \cap \mathcal{B}_1 \right\rbrace.
\]
Record the solution $( x^{\nu}, \theta_1^{\nu}, \theta_2^{\nu},\ldots,
\theta_{|S|}^{\nu} )$.
\item[Step 2]
Evaluate $\mathcal{Q}(x^{\nu}) = \sum_{s \in S} p_s Q(x^{\nu}, \xi_s)$.
		
		$\mathcal{Q}(x^{\nu}) < \infty$?
\end{description}
	
\end{frame}

\begin{frame}
\frametitle{Multicut $L$-shaped method: step 2}
	
\begin{itemize}
\item
If $\mathcal{Q}(x^{\nu}) = \infty$, there is a realization $\xi_k$ for which $Q(x^{\nu}, \xi_k) = \infty$, we add a feasibility cut and set
\[
\mathcal{B}_1 := \mathcal{B}_1 \cap \lbrace
(x,\theta_1,\theta_2,\ldots,\theta_{|S|}) \,|\, \sigma^T(h(\xi_k) -
T(\xi_k)x) \geq 0\rbrace.
\]
Note: this inequality is the same than in the simple-cut version.\\
Return to Step 1.
\item
If $\mathcal{Q}(x^{\nu}) < \infty$, we are able to solve all the second-stage problems, for all the scenarios, and obtain the corresponding dual solutions $\hat{\pi}_s$.
We get the subgradients
\[
u = -T_s^T\hat{\pi}_s \in \partial Q(x^{\nu}, \xi_s).
\]
\end{itemize}
	
\end{frame}

\begin{frame}
\frametitle{Multicut $L$-shaped method: step 2 - $\mathcal{Q}(x^{\nu}) < \infty$}
	
\begin{itemize}
\item
If $p_s Q(x^{\nu}, \xi_s) \leq \theta_s$, $\forall\, s \in S$, stop: $x^{\nu}$ is optimal.
\item
If $p_s Q(x^{\nu}, \xi_s) > \theta_s$,
\[
\mathcal{B}_1 := \mathcal{B}_1 \cap \lbrace (x,\theta_1,\theta_2,\ldots,\theta_{|S|}) \,|\,
\theta_s \geq p_s Q(x^{\nu}, \xi_s) + p_s u^T(x-x^{\nu})\rbrace.
\]
		
Return to Step 1.
\end{itemize}
	
\end{frame}

\begin{frame}
\frametitle{Hybrid approaches}

\begin{itemize}
	\item
	Partition the scenarios in $C$ ``clusters'' $\mathcal{S}_1$,
	$\mathcal{S}_1$, \ldots, $\mathcal{S}_C$, and define
	\[
	\mathcal{Q}_{[\mathcal{S}_k]}(x) = \sum_{s \in \mathcal{S}_k} p_s Q(x, \xi_s).
	\]
	Thus
	\[
	\mathcal{Q}(x) = \sum_{k = 1}^C \mathcal{Q}_{[\mathcal{S}_k]}(x)
	\]
	\[
	-\sum_{s \in \mathcal{S}_k} p_s(T(\xi_s))^T\hat{\pi}_s
	\in \partial \mathcal{Q}_{[\mathcal{S}_k]}(x).
	\]
	\item
	We can therefore do the same: approximate $\mathcal{Q}_{[\mathcal{S}_k]}(x)$ with the subgradient inequality.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Algorithm}

\begin{description}
\item[Step 0]
Let $\theta_k^0$ be a lower bound for $\mathcal{Q}_{[\mathcal{S}_k]}(x)$.
Set
\begin{align*}
\mathcal{B}_0 &= \lbrace \RR_n^+ \times \lbrace \theta_1,
\theta_2,\ldots, \theta_C \rbrace \,|\, Ax = b \rbrace, \\
\mathcal{B}_1 &= \lbrace \RR_n^+ \times \lbrace \theta_1,
\theta_2,\ldots, \theta_C \rbrace \,|\, \theta_k \geq \theta_k^0 \rbrace.
\end{align*}
Set $\nu = 0$.
\item[Step 1]
Increment $\nu$ and solve the {\blue master problem}
\[
\min c^Tx + \sum_{k = 1}^C \theta_k \mbox{ t.q. } (x, \theta_1,
\theta_2,\ldots,  \theta_C) \in \mathcal{B}_0 \cap \mathcal{B}_1.
\]
Let $(x^{\nu}, \theta_1^{\nu}, \theta_2^{\nu},\ldots,
\theta_C^{\nu})$ be the found solution.
\end{description}

\end{frame}

\begin{frame}
\frametitle{Algorithm - Step 2}

\begin{itemize}
\item
Evaluate $\mathcal{Q}(x^{\nu}) = \sum_{s \in S} p_s Q(x^{\nu}, \xi_s)$.
\item
If $\mathcal{Q}(x^{\nu}) = \infty$, there is some $\xi_k$ such that $Q(x^{\nu}, \xi_k) = \infty$.
We add a {\red feasibility cut}:
\[
\mathcal{B}_1 := \mathcal{B}_1 \cap \left\lbrace (x, \theta) \,|\,
\sigma^T(h(\xi_k)-T(\xi_k)x) \leq 0 \right\rbrace.
\]
This inequality has no term in $\theta_s$; it is the same inequality than in the standard L-Shaped method.

Go back to Step 1.
\item
If $\mathcal{Q}(x^{\nu}) < \infty$, the $S$ LP's corresponding to the $S$ scenarios can be solved, with the dual solutions $\pi_s^{\nu}$, and
\[
u_k := -\sum_{s \in \mathcal{S}_k} p_s(T(\xi_s))^T\hat{\pi}_s
\in \partial \mathcal{Q}_{[\mathcal{S}_k]}(x).
\]
If $\mathcal{Q}_{[\mathcal{S}_k]}(x) \leq \theta_k$, $k = 1,\ldots,C$, stop: $x^{\nu}$ is optimal.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Algorithm - Step 2 (cont'd)}

For each $k$ such that  $\mathcal{Q}_{[\mathcal{S}_k]}(x) >
\theta_k$, add an optimality cut:
\[
\mathcal{B}_1 := \mathcal{B}_1 \cap \left\lbrace (x, \theta_1,
\theta_2,\ldots, \theta_C) \,|\, \theta_k \geq
\mathcal{Q}_{[\mathcal{S}_k]}(x^{\nu}) + u_k^T(x-x^{\nu}) \right\rbrace.
\]

%\mbox{}

%Ok, all right,\ldots but what happens if we start from the optimal solution $x^*$?

\end{frame}

\begin{frame}
\frametitle{Example}

{\red Birge and Louveaux, Section~5.1, Exercice 5, page 200}

\mbox{}

Assume $n_1 = 1$ (dimension of $x$), $m_1 = 0$ (no first-stage constraint), $m_2 = 3$ (three second-stage constraints), $n_2 = 6$ (dimension of $y$), and
\[
W = \begin{pmatrix}
1 & -1 & -1 & -1 & 0 & 0 \\
0 & 1 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 & 1
\end{pmatrix}.
\]
Assume moreover $S = 2$, and that the two realizations of $\bsxi$ have the same probability.
These realizations are
\begin{align*}
\xi^1 &= (q^1, h^1, T^1)^T, \\
\xi^2 &= (q^2, h^2, T^2)^T.
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Example (cont'd)}

Assume
\begin{align*}
& q^1 = (1,0,0,0,0,0)^T,\ q^2 = (3/2, 0, 2/7, 1, 0, 0)^T, \\
& h^1 = (-1,2,7)^T,\ h^2 = (0,2,7)^T, \\
& T^1 = T^2 = (1,0,0)^T.
\end{align*}

\mbox{}

For the first value $\xi^1$, we have thus the second-stage program
\begin{align*}
\min_y\ & y_1 \\
\mbox{s.t. } & y_1 - y_2 - y_3 -y_4 = -1 - x, \\
& y_2 + y_5 = 2, \\
& y_3 + y_6 = 7, \\
& y \geq 0.
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Example (cont'd)}

It is easy to see that, for $\xi^1$,
\[
Q_1(x) = \begin{cases}
-x-1 & \mbox{if } x \leq -1, \\ 0 & \mbox{if } x \geq -1.
\end{cases}.
\]
For the second value of $Q(x,\xi)$, we can show that
\[
Q_2(x) = \begin{cases}
-1.5x & \mbox{if } x \leq 0, \\
0 & \mbox{if } 0 \leq x \leq 2, \\
\frac{2}{7}(x-2) & \mbox{if } 2 \leq x \leq 9, \\
x - 7 & \mbox{if } x \geq 9.
\end{cases}
\]

Assume also that $-20 \leq x \leq 20$ and $c = 0$.

\mbox{}

What are the (expected) properties that we have for $Q_1(x)$ and
$Q_2(x)$?\\
What about the recourse?

\end{frame}

\begin{frame}
\frametitle{Example: $L$-shaped}

Start from $x = -2$. We have the following sequence of iterates
\begin{description}
\item[Iteration 1] $x^1 = -2$, $\theta^1$ is omitted and we add the cut
\[
\theta \geq -0.5 -1.25x.
\]
\item[Iteration 2] $x^2 = 20$, $\theta^2 = -25.5$, new cut:
\[
\theta \geq 0.5x - 3.5.
\]
\item[Iteration 3] $x^3 = 12/7$, $\theta^3 = -37/14$, new cut:
\[
\theta \geq= 0.
\]
\item[Iteration 4] $x^4 \in [-2/5, 7]$, $\theta^4$.
If $x^4$ is chose as any value in $[0,2]$, stop.
\end{description}

\end{frame}

\begin{frame}
\frametitle{Example: multicut $L$-shaped}

The multicut approach would give the following sequence.
\begin{description}
\item[Iteration 1] $x^1 = -2$, $\theta^1$ and $\theta^2$ are omitted;
we add the cuts $\theta_1 \geq -0.5 - 0.5x,\ \theta_2 \geq -3/4x$.
Note: $\theta_1 \geq \frac{1}{2} (-x-1)$ and $\theta_1 \geq \frac{1}{2} (-1.5x)$
\item[Iteration 2] $x^2 = 20$, $\theta_1^2 = -10.5$, $\theta_2^2 =
-15$; new cuts: $\theta_1 \geq 0$, $\theta_2 \geq 0.5x - 3.5$.
\item[Iteration 3] $x^3 = 2.8$, $\theta_1^3 = 0$, $\theta_2^3 = -2.1$;
new cut: $\theta_2^3 \geq 1/7(x-2)$.
\item[Iteration 4] $x^4 = 0.32$, $\theta_1^4 = 0$, $\theta_2^4 =
-0.24$; new cut: $\theta_2 \geq 0$.
\item[Iteration 5] $x^5 = 0$, $\theta_1^5 = \theta_2^5 = 0$. Stop.
\end{description}

\mbox{}

This example shows the the multicut approach can sometimes take more iterations than the single cut version.

\end{frame}

\begin{frame}
\frametitle{Regularization}

The $L$-shaped method suffers from ``stability'' issues,
\begin{itemize}
\item
especially during the first iterations when we do not have a good model of $\mathcal{Q}(x)$;
\item
especially when we start from a good estimate of the solution.
\end{itemize}
It is therefore useful to {\red regularize} the method. Various techniques exist.

\mbox{}

{\blue Some references} on the stabilization of the Benders decomposition:
\begin{itemize}
\item
Marsten, Hogan, Blankenship (1975);
\item
Lemaréchal (1975,\ldots);
\item
Kiwiel (1983,\ldots);
\item
Neame, Boland, and Ralph (1998).
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Regularization of the $L$-Shaped method}

\vspace*{-3mm}

\begin{itemize}
\item
{\blue Penalization of the step length}: A. Ruszczy\'nski, {\sl A
Regularized Decomposition for Minimizing a Sum of Polyhedral
Functions}, Mathematical Programming 35, pp. 309--333, 1986.
\item
{\blue Approach in $\|\cdot\|_\infty$}: J. T. Linderoth and S. J. Wright, {\sl
Implementing a Decomposition Algorithm for Stochastic Programming on a
Computational Grid}, Computational Opimization and Applications 24,
pp. 207--250, 2003.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Penalization of the step length}

At iteration $k$, we solve the program
\[
\min_x c^Tx + \sum_{j \in C} \theta_j + \frac{1}{2\rho} \| x-x_k \|^2.
\]
\begin{itemize}
\item
Large $\rho$: as the $L$-shaped.
\item
Small $\rho$: we do not move a lot.
\end{itemize}
We speak of {\blue regularized decomposition method}.

\end{frame}

\begin{frame}
\frametitle{Trust-region approach}

The principle is also to penalize the step length.
At iteration $k$, we require $\| x-x_k \|_k \leq \Delta_k$, where $\| \cdot \|_k$ designs an usual norm (typically 2 or $\infty$).

\begin{itemize}
\item
Big $\Delta_k$: as the $L$-shaped.
\item
Small $\Delta_k$: we do not move a lot.
\end{itemize}

We speak of the {\blue trust-region regularization}.

\mbox{}

In both case, we see the introduction of {\blue nonlinear programming} concepts.

\end{frame}

\begin{frame}
\frametitle{Summary}

Let $m(x)$ be the ``model'' used as first-stage problem.
\begin{itemize}
\item
{\red Multicut $L$-shaped}
\[
m(x) = \min_x \Big\lbrace c^Tx + \sum_{k = 1}^C \theta_k \,|\, (x, \theta_1,
\theta_2,\ldots, \theta_C) \in \mathcal{B}_0 \cap \mathcal{B}_1
\Big\rbrace.
\]
\item
{\red Trust-region}
\begin{align*}
m(x)
= \min_x \Big\lbrace &  c^Tx + \sum_{k = 1}^C \theta_k \,|\, (x, \theta_1,
\theta_2,\ldots, \theta_C) \in \mathcal{B}_0 \cap \mathcal{B}_1, \\
& \| x-x^l \|_{\infty} \leq \Delta_l \Big\rbrace.
\end{align*}
\item
{\red Regularized decomposition}
\begin{align*}
m(x) = \min_x \Big\lbrace & c^Tx + \sum_{k = 1}^C \theta_k + (1/2\rho_l)
\| x-a^l \|_2^2 \\ & \mbox{s.t. } (x, \theta_1, \theta_2,\ldots,
\theta_C) \in \mathcal{B}_0 \cap \mathcal{B}_1, \Big\rbrace.
\end{align*}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Regularized decomposition method}

{\red Reference: Birge and Louveaux, Section~5.2}. Multicut version.

\begin{description}
\item[Step 0]
Set $\rho = 1$, $r = \nu = 0$, $s(k) = 0$ for all $k = 1,\ldots,S$.
Select a feasible solution $a^1$.
\item[Step 1]
Set $\nu := \nu+1$, and solve the regularized master program
\begin{align*}
\min\ & c^Tx + \sum_{k = 1}^S \theta_k + \frac{1}{2\rho} \| x - a^{\nu}
\|^2,\\
\mbox{s.t. } & Ax = b,\\
& D_{\ell}^Tx \geq d_{\ell},\ \ell = 1,\ldots,r, \\
& -\beta^T_{\ell(k)} x + \theta_k \geq \alpha_{\ell(k)},\ \ell(k) =
1,\ldots,s(k),\\
& x \geq 0.
\end{align*}
\end{description}

\end{frame}

\begin{frame}
\frametitle{Regularized decomposition method (cont'd)}

\begin{description}
\item[Step 1] (cont'd)
Let $(x^{\nu}, \theta^{\nu})$ be an optimal solution of this program, where $(\theta^{\nu})^T = (\theta^{\nu}_1,\ldots,\theta^{\nu}_K)^T$.

If $s(k) = 0$ for some $k$, $\theta_k^{\nu}$ is ignored in the computation.
If $c^Tx^{\nu} + e^T\theta^{\nu} = c^Ta^{\nu} + \mathcal{Q}(a^{\nu})$, stop: $a^{\nu}$ is optimal.
\item[Step 2]
If a feasibility cut is generated, set $a^{\nu+1} = a^{\nu}$
(unfeasible null step), and return to Step 1.
\item[Step 3]
For $k = 1,\ldots,S$, solve the linear subproblem
\begin{align*}
\min_y &\ q_k^Ty \\
\mbox{s.t. } & Wy = h_k - T_kx^{\nu}, \\
& y \geq 0.
\end{align*}
Compute $\mathcal{Q}_k(x^{\nu})$.
\end{description}

\end{frame}

\begin{frame}
\frametitle{Regularized decomposition method (cont'd)}

\begin{description}
\item[Step 3] (cont'd) If
\[
\theta_k^{\nu} < p_k (\pi_k^{\nu})^T(h_k - T_kx^{\nu}),
\]
add an optimality cut of the form
\[
-\beta^T_{\ell(k)} x + \theta_k \geq \alpha_{\ell(k)}.
\]
Set $s(k) = s(k)+1$.
\item[Step 4] If for all $k$,
\[
\theta_k^{\nu} \geq p_k (\pi_k^{\nu})^T(h_k - T_kx^{\nu}),
\]
set $a^{\nu+1} := x^{\nu}$ (exact serious step). Return to Step 1.
\end{description}

\end{frame}

\begin{frame}
\frametitle{Regularized decomposition method (cont'd)}

\begin{description}
\item[Step 5] If
\[
c^Tx^{\nu} + \mathcal{Q}(x^{\nu}) \leq c^Ta^{\nu} + \mathcal{Q}(a^{\nu}),
\]
set $a^{\nu+1} := x^{\nu}$ (serious approximative step).

Otherwise, set $a^{\nu+1} := a^{\nu}$.

Return to Step 1.
\end{description}

\mbox{}

{\blue Immediate extension}: allow $\rho$ to vary.

\mbox{}

The implementation of the regularized $L$-shaped raises various practical questions.
An efficient implementation is proposed in Ruszczy\'nski (1986).
\end{frame}

\begin{frame}
\frametitle{Example: regularized $L$-shaped}

Consider again {\red Birge and Louveaux, Section~5.1, Exercice 5, page 200}.

Take $a^1 = -0.5$ as starting point.
This point corresponds to the solution of the expected value problem. i.e the problem obtained by replacing $\bsxi$ by $E[\xi]$.
The second stage program becomes
\begin{align*}
\min\ & \frac{5}{4}y_1 + \frac{1}{7}y_3 + \frac{1}{2}y_4\\
\mbox{s.t. } & y_1 - y_2 - y_3 - y_4 = -\frac{1}{2}-x, \\
& y_2+y_5 = 2, \\
& y_3+y_6 = 7, \\
& y \geq 0.
\end{align*}

\mbox{}

{\blue Exercise}: check that $a^1 = -0.5$ is indeed the optimal value of this program (using the KKT/primal-dual conditions).
\end{frame}

\begin{frame}
\frametitle{Example: regularized $L$-shaped (cont'd)}

For $a^1 = -0.5$, the regularized master problem has also for solution $x^1 = -0.5$, and the second stage program of the expected-value problem has the solution $(0,0,0,0,2,7)$, while $Q_1(a^1) = 0$, $Q_2(a^1) = 3/4$, so $\mathcal{Q}(a^1) = 3/8$.
The regularized approach leads to the following iterations.
\begin{description}
\item[Iteration 1] The cuts $\theta_1 \geq 0$, $\theta_2 \geq -3/4
x$ are added. $a^2:= a^1$.
\item[Iteration 2]
The regularized master program is
\begin{align*}
\min\ & \theta_1 + \theta_2 + \frac{1}{2}(x+0.5)^2 \\
\mbox{s.t. } & \theta_1 \geq 0,\ \theta_2 \geq -\frac{3}{4}x,
\end{align*}
with the solution $x^2 = 0.25$, $\theta_1 = 0$, $\theta_2 = -3/16$.
A new cut $\theta_2 \geq 0$ is added.
As $\mathcal{Q}(0.25) = 0 < \mathcal{Q}(a^1)$, $a^3 := 0.25$ (serious approximate step).
\end{description}
\end{frame}

\begin{frame}
\frametitle{Example: regularized $L$-shaped (cont'd)}

\begin{description}
\item[Iteration 3]
The regularized master problem is
\begin{align*}
\min\ & \theta_1 + \theta_2 + \frac{1}{2}(x-0.25)^2 \\
\mbox{s.t. } & \theta_1 \geq 0,\ \theta_2 \geq -\frac{3}{4}x, \theta_2
\geq 0,
\end{align*}
with the solution $x^3 = 0.25$, $\theta_1 = 0$, $\theta_2 = 0$.
As $\theta^{\nu} = \mathcal{Q}(a^{\nu})$, a solution is found.
\end{description}

\begin{theo}
If the original problem has a solution, the algorithm stops after a finite number of iterations.
Otherwise, it generates a sequence of feasible points $\lbrace a^{\nu} \rbrace$ such that
$\mathcal{Q}(a^{\nu}) \rightarrow -\infty$, as $\nu \rightarrow \infty$.
\end{theo}
\begin{proof}
See Birge and Louveaux, Section~5.2.
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Regularization using trust-region methods}

{\blue Basic philosophy} of trust-region methods: replace the original problem with some locally valid approximation easier to handle.

\mbox{}

\begin{center}
\begin{minipage}[t]{0.49\textwidth}
\centering
\textcolor{red}{Original problem}
\begin{align*}
	\min_x \ & c^T x + \cQ(x) \\
	\mbox{s.t. } & Ax = b \\
	& x \geq 0.
\end{align*}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
\centering
\textcolor{red}{Model}
\begin{align*}
	\min_{x, \theta} \ & m(x, \theta) := c^T x + \theta \\
	\mbox{s.t. } & Ax = b \\
	& {\rm feasibility\ cuts} \\
	& {\rm optimality\ cuts} \\
	& x \geq 0.
\end{align*}
\end{minipage}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Regularization using trust-region methods (cont'd)}

\begin{itemize}
	\item The model $m(\cdot)$ is valid locally only.
	\item Constrain the candidate iterate to stay in a trust region $\calB_k$ centered at the current solution $x_k$:
$$
\cB_k = \{ x \,|\, \| x-x_k \|_k \leq \Delta_k \},
$$
where $\Delta_k$ is the trust-region radius, and $\|\cdot\|_k$ is the norm used at iteration $k$.
\item
In the context a linear programming, it is common to use $\infty$-norm as
$$
\| x-x_k \|_{\infty} \leq \Delta_k \text{ iff } -\Delta_k \bse \leq x-x_k \leq \Delta_k \bse.
$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Regularization using trust-region methods (cont'd)}

\begin{itemize}
	\item 
The trust-region subproblem at iteration $k$ is now
\begin{align*}
	\min_{x,\theta} \ & m(x) := c^T x + \theta \\
	\mbox{s.t. } & Ax = b \\
	& {\rm feasibility\ cuts} \\
	& {\rm optimality\ cuts} \\
	& \|x-x_k\|_{\infty} \leq \Delta_k\\
	& x \geq 0.
\end{align*}
\item
We can extend the model to multi cuts or scenarios partition in clusters.
\item
Only move if the model does perform well.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Regularization using trust-region methods (cont'd)}

\begin{itemize}
	\item 
Let $x^*$ be the first-stage solution.
\item
Set $0 < \eta_1 \leq \eta_2 < 1$. For instance, $\eta_1 = \eta_2 = 10^{-4}$.
\item
Compare the actual reduction to the prediction reduction:
\begin{align*}
	\rho = \frac{c^T(x_k-x^*)+\cQ(x_k)-\cQ(x^*)}{m(x_k)-m(x^*)}
\end{align*}
\item
\textcolor{blue}{Unsuccessful iteration}: if $\rho < \eta_1$, set $x_{k+1} = x_k$, $\Delta_{k+1} \leq \Delta_k$, add cuts.
Enforce $\Delta_{k+1} < \Delta_k$ if $\Delta_k$ has been kept the same for several consecutive unsuccessful iterations or if $\rho \Delta_k$ it too large.
\item
\textcolor{blue}{Successful iteration}: if $\rho \geq \eta_1$, set $x_{k+1} = x^*$.
\item
\textcolor{blue}{Very successful iteration}: if $\rho \geq \eta_2$, set $\Delta_{k+1} = \min \{ \Delta_{\max}, \beta \Delta_k \}$, with $\beta > 1$.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Regularization using trust-region methods: notes}

Additional details and considerations in Linderoth and Wright.
\begin{itemize}
\item
Allow deletion of inactive cuts.
\item
Investigate parallelization of the method.
\item
Do not distinguish successful and very successful iterations: $\eta_1 = \eta_2 \in (0, 1/2)$.
\end{itemize}

\end{frame}

\end{document}